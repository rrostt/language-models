{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "data = WikiText103(root='data', split='train')\n",
    "# data = WikiText2(root='data', split='train')\n",
    "# loader = torch.utils.data.DataLoader(data, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105028371"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i, text in enumerate(data):\n",
    "    line = text.replace('  ', ' ')\n",
    "    if len(line) > 0:\n",
    "        words += list(filter(len, line.split(' '))) + ['\\n']\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (122675958 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n = Valkyria Chronicles III = \\n \\n \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-22 22:21:59\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# print timestamp\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33278"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab from file if it exists\n",
    "# otherwise create vocab from words\n",
    "# and save\n",
    "vocab = []\n",
    "if os.path.exists('vocab.txt'):\n",
    "    with open('vocab.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            vocab.append(line.strip())\n",
    "else:\n",
    "  # create vocab from words\n",
    "  for i, word in enumerate(words):\n",
    "      if word not in vocab:\n",
    "          vocab.append(word)\n",
    "  # save vocab to file\n",
    "  with open('vocab.txt', 'w') as f:\n",
    "      for word in vocab:\n",
    "          f.write(word + '\\n')\n",
    "\n",
    "# get unique characters in string text\n",
    "#vocab = tuple(set(words))\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267735"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wf  = {}\n",
    "# for w in words:\n",
    "#     if w in wf:\n",
    "#         wf[w] += 1\n",
    "#     else:\n",
    "#         wf[w] = 1\n",
    "# len(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [char2int[ch] for ch in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq, feature)\n",
    "class PositionEncoding(torch.nn.Module):\n",
    "  def __init__(self, max_length, embed_size):\n",
    "    super(PositionEncoding, self).__init__()\n",
    "    self.max_length = max_length\n",
    "    self.embed_size = embed_size\n",
    "\n",
    "    pos = torch.arange(0, max_length).unsqueeze(1)\n",
    "    args = pos / (10000 ** (2 * torch.arange(0, embed_size, 2) / embed_size))\n",
    "    self.pe = torch.zeros((max_length, embed_size))\n",
    "    self.pe[:, ::2] = torch.sin(args)\n",
    "    self.pe[:, 1::2] = torch.cos(args)\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.pe = self.pe.to(x.device)\n",
    "    return x + self.pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, k, v: (batch, seq_len, embed_size)\n",
    "def attention(q, k, v, mask=None):\n",
    "  qk = q @ k.transpose(-1, -2) / (k.shape[-1] ** 0.5)\n",
    "  if mask is not None:\n",
    "    qk = qk + mask\n",
    "  weights = torch.softmax(qk, dim=-1)\n",
    "  return weights @ v\n",
    "\n",
    "# (batch, seq, feature)\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "  def __init__(self, embed_size, heads, max_length):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_size = embed_size // heads\n",
    "    self.max_length = max_length\n",
    "\n",
    "    self.mask = torch.zeros((max_length, max_length))\n",
    "    ix = torch.triu_indices(max_length, max_length, 1)\n",
    "    self.mask[ix[0], ix[1]] = -1e9\n",
    "\n",
    "    self.Wq = torch.nn.Linear(embed_size, embed_size)\n",
    "    self.Wk = torch.nn.Linear(embed_size, embed_size)\n",
    "    self.Wv = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    self.Wo = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, s, e = x.shape\n",
    "    self.mask = self.mask.to(x.device)\n",
    "    # x is (batch, seq_len, embed_size)\n",
    "    q = self.Wq(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    k = self.Wk(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    v = self.Wv(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    x = attention(q, k, v, self.mask)\n",
    "    # x is now (heads, seq_len, head_size)\n",
    "    x = x.transpose(1, 2).contiguous().view(b, s, self.embed_size)\n",
    "    return self.Wo(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq, feature)\n",
    "class FeedForward(torch.nn.Module):\n",
    "  def __init__(self, embed_size):\n",
    "    super(FeedForward, self).__init__()\n",
    "    self.main = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embed_size, embed_size * 4),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(embed_size * 4, embed_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "  def __init__(self, embed_size, heads, max_length):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "\n",
    "    self.attention = MultiHeadAttention(embed_size, heads, max_length)\n",
    "    self.norm1 = torch.nn.LayerNorm(embed_size)\n",
    "    self.norm2 = torch.nn.LayerNorm(embed_size)\n",
    "    self.ff = FeedForward(embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    attended = self.attention(x)\n",
    "    x = self.norm1(attended + x)\n",
    "    fed = self.ff(x)\n",
    "    x = self.norm2(fed + x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(torch.nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, depth, heads, max_length):\n",
    "    super(LLM, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.max_length = max_length\n",
    "    self.depth = depth\n",
    "    self.heads = heads\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_enc = PositionEncoding(max_length, embed_size)\n",
    "    self.decoder = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    blocks = [TransformerBlock(embed_size, heads, max_length) for _ in range(depth)]\n",
    "    self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    for p in self.parameters():\n",
    "      if p.dim() > 1:\n",
    "        nn.init.xavier_normal_(p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    # position encoding\n",
    "    x = self.pos_enc(x)\n",
    "    # feed through transformer blocks\n",
    "    x = self.blocks(x)\n",
    "    x = self.decoder(x)\n",
    "    return x # torch.softmax(out, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "  def __init__(self, optimizer, embed_size, warmup_steps, last_epoch=-1):\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.embed_size = embed_size\n",
    "    self._step = 0\n",
    "    super(WarmupScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "  def get_lr(self):\n",
    "    self._step += 1\n",
    "    return [self.embed_size ** -0.5 * min(self._step ** -0.5, self._step * self.warmup_steps ** -1.5) for group in self.optimizer.param_groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 70.427729M\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size # len(vocab)\n",
    "embed_size = 512\n",
    "depth = 6\n",
    "heads = 4\n",
    "max_length = 256\n",
    "batch_size = 32\n",
    "model = LLM(vocab_size, embed_size, depth, heads, max_length)\n",
    "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model size: {model_size / 1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, model, optim, loss, epoch):\n",
    "  torch.save({'optimizer': optim.state_dict(), 'model': model.state_dict(), 'loss': loss, 'epoch': epoch}, filename)\n",
    "\n",
    "def load_checkpoint(filename, model, optim):\n",
    "  checkpoint = torch.load(filename)\n",
    "  model.load_state_dict(checkpoint['model'])\n",
    "  optim.load_state_dict(checkpoint['optimizer'])\n",
    "  loss = checkpoint['loss']\n",
    "  epoch = checkpoint['epoch']\n",
    "  return model, optim, loss, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latest(model, optim, loss, epoch):\n",
    "  save_checkpoint(f'ckpt/e-{epoch}.pt', model, optim, loss, epoch)\n",
    "\n",
    "def load_latest(model, optim):\n",
    "  files = glob.glob('ckpt/*.pt')\n",
    "  files.sort()\n",
    "  if len(files) > 0:\n",
    "    model, optim, loss, epoch = load_checkpoint(files[-1], model, optim)\n",
    "    print(f\"Loaded checkpoint {files[-1]}\")\n",
    "    return model, optim, loss, epoch\n",
    "  else:\n",
    "    return model, optim, 0, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def generate(model, encoded_prompt, length):\n",
    "#  model.eval()\n",
    "  x = torch.tensor(encoded_prompt).unsqueeze(0).to('cuda')\n",
    "  with torch.no_grad():\n",
    "    for i in range(length):\n",
    "      y = model(x[0, -max_length:])\n",
    "      y = torch.softmax(y, dim=-1)\n",
    "      y = torch.multinomial(y[0, -1, :], 1).unsqueeze(0)\n",
    "      x = torch.cat([x, y], dim=1)\n",
    "  return tokenizer.decode(x[0]), tokenizer.decode(encoded_prompt)\n",
    "  # return ' '.join([vocab[i] for i in x[0]]), ' '.join([vocab[i] for i in encoded_prompt])\n",
    "\n",
    "def train(model, optim, loss_fn, data, epochs=10, device=\"cpu\", start_epoch=0):\n",
    "  # model.to(device)\n",
    "  print(len(data))\n",
    "  lossi = []\n",
    "  for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    for i in range((len(data) // max_length // batch_size) + 1):\n",
    "      ix = torch.randint(0, len(data) - max_length - 1, (batch_size,)) if len(data) != max_length * batch_size + 1 else torch.zeros((batch_size,), dtype=torch.int64)\n",
    "      x = torch.tensor([data[i:i + max_length] for i in ix]).to(device)\n",
    "      y = torch.tensor([data[i + 1:i + max_length + 1] for i in ix]).to(device)\n",
    "      if y.shape[1] != max_length:\n",
    "        continue\n",
    "      y_hat = model(x)\n",
    "      loss = loss_fn(y_hat.view(-1, y_hat.shape[-1]), y.view(-1))\n",
    "      lossi.append(loss.item())\n",
    "      optim.zero_grad()\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "      # if len(lossi)%100 == 0:\n",
    "      #   print(f\"epoch {epoch} i {i} loss {sum(lossi)/len(lossi)}\")\n",
    "    output, input = generate(model, data[:max_length], 12)\n",
    "    output = output[len(input):]\n",
    "    print(f\"epoch {epoch} loss {sum(lossi)/len(lossi)}: {output}\")\n",
    "    save_latest(model, optim, sum(lossi)/len(lossi), epoch)\n",
    "    lossi = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint ckpt/e-9.pt\n",
      "last epoch: 9 loss: 4.901571885770203\n"
     ]
    }
   ],
   "source": [
    "model = LLM(vocab_size, embed_size, depth, heads, max_length)\n",
    "model.to('cuda')\n",
    "model, optim, loss, last_epoch = load_latest(model, optim)\n",
    "print(f\"last epoch: {last_epoch} loss: {loss}\")\n",
    "# last_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122675958\n",
      "epoch 10 loss 4.903378900586286:  are visual ones other elements. Blair Bollour was the second\n",
      "epoch 11 loss 4.90201328548356:  evening, and ignored their feedback to Seatstone sportsized.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# last_epoch = -1\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train(model, optim, loss_fn, encoded[:], epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m, start_epoch\u001b[39m=\u001b[39;49mlast_epoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [17], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, loss_fn, data, epochs, device, start_epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y_hat\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m lossi\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     30\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# last_epoch = -1\n",
    "train(model, optim, loss_fn, encoded[:], epochs=100, device=\"cuda\", start_epoch=last_epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input: \n",
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      " \n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received\n",
      "output: \n",
      " \n",
      " = – = = \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " <unk> <unk> @-@ is <unk> <unk> <unk> <unk> ) <unk> <unk> <unk> \n",
      " \n",
      " 't the Battlefield 94 : is the been to the a @-@ . . the . and a <unk> <unk> of <unk> . . . in the . the are the world Portable . \n",
      " in the 2010 , the , the was the first game game the PlayStation @-@ of \n",
      " , game game of the game the , ray , , the most is and player of from to the player game . the the player <unk> \" . and \" \" @-@ , @-@ series . the . the time Series Trek . had . . . . the the . the series Embassy . . \" . . \n",
      " \n",
      " \n",
      " series was to of the , and the the series amount of the series of in the Chronicles II . The the was the player game the the series of and was been a puzzles , and as a the player 's of , the . . \n",
      " of , , , <unk> Hitoshi <unk> , <unk> to the game . and with the Chronicles II and , Ozawa . \n",
      " review game , the , the film as \n",
      " film was first game was released by the 'n in \n",
      " \n",
      " \n",
      " was as the of of the , and the released the the the and the <unk> . The the of the was a\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "input = encoded[0:256]\n",
    "output = model(torch.tensor(input).to(\"cuda\").unsqueeze(0))\n",
    "token_output = torch.softmax(output, dim=-1).argmax(dim=-1)[0]\n",
    "print(f' input: {\" \".join([vocab[i] for i in input])}')\n",
    "print(f'output: {\" \".join([vocab[i] for i in token_output])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model-word-12layers-e688.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      " \n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received <unk> delivered videotape as <unk> , and Nicholas <unk> , and <unk> <unk> from this way against them for designing their audiovisuals . They targeted the song a corrupt Business and developer were designing Magadheera could be designed as straight questions to greater advances . It was increased not be inspired by licensed to being sold by the game . A response to a number of squares from number piece , and downed with the single refers <unk> by <unk> ( released Mercy & Drive , and Perry called out when they <unk> proposal and Utsler . \n",
      " \n",
      " \n",
      " \n",
      " = = Legacy = = \n",
      " \n",
      " \n",
      " \n",
      " The Amps <unk> ( The Goat ) is impersonating broadcast for his poorly episode change that has almost been delaying , either complete . <unk> is four Kick and information for Toei chrome stars and 5 inches ( 4 @.@ 1 @.@ 4 in Fulton 's Road but from all time of 2004 . A Primetime season in a Bormanis , the Secret needed potentially also IAU and made the film was performance . \n",
      " \n",
      " The designers in shortly ( Richard Fleming 's special distributor Belt Guide game of Development ( substance <unk> ) , comic from 15 on 6 @.@ Mike 360 in a beta panel . \n",
      " \n",
      " <unk> was warmly made to develop players and has already been converted across \" Christians due to the enhanced life that aired through a <unk> due to paint . Although <unk> gave Mandel , are\n"
     ]
    }
   ],
   "source": [
    "output, input = generate(model, encoded[0:0+max_length], 256)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder():\n",
    "  None\n",
    "\n",
    "def gen_noise():\n",
    "  None\n",
    "\n",
    "def unet():\n",
    "  None\n",
    "\n",
    "def vae_decode():\n",
    "  None\n",
    "\n",
    "cfg = 3.5\n",
    "timesteps = []\n",
    "prompt = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoding = text_encoder(prompt)\n",
    "latent_image = gen_noise(1000)\n",
    "\n",
    "for t in timesteps:\n",
    "  cond_noise = unet(latent_image, t, text_encoding)\n",
    "  ucond_noise = unet(latent_image, t)\n",
    "\n",
    "  noise = ucond_noise + cfg * (cond_noise - ucond_noise)\n",
    "\n",
    "  latent_image = latent_image - noise + gen_noise(t)\n",
    "\n",
    "image = vae_decode(latent_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
