{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "data = WikiText103(root='data', split='train')\n",
    "loader = torch.utils.data.DataLoader(data, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105028371"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i, w in enumerate(data):\n",
    "    words += w.split(' ')\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text file into data\n",
    "with open('data/text8', 'r') as f:\n",
    "    data = f.read()\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data # '\\n'.join(list(data))\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique characters in string text\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [char2int[ch] for ch in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq, feature)\n",
    "class PositionEncoding(torch.nn.Module):\n",
    "  def __init__(self, max_length, embed_size):\n",
    "    super(PositionEncoding, self).__init__()\n",
    "    self.max_length = max_length\n",
    "    self.embed_size = embed_size\n",
    "\n",
    "    pos = torch.arange(0, max_length).unsqueeze(1)\n",
    "    args = pos / (10000 ** (2 * torch.arange(0, embed_size, 2) / embed_size))\n",
    "    self.pe = torch.zeros((max_length, embed_size))\n",
    "    self.pe[:, ::2] = torch.sin(args)\n",
    "    self.pe[:, 1::2] = torch.cos(args)\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.pe = self.pe.to(x.device)\n",
    "    return x + self.pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(3, 4, 2, 2)\n",
    "k = torch.randn(3, 4, 2, 2)\n",
    "v = torch.randn(3, 4, 2, 2)\n",
    "\n",
    "qk = q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5)\n",
    "weights = torch.softmax(qk, dim=-1)\n",
    "(weights @ v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, k, v: (batch, seq_len, embed_size)\n",
    "def attention(q, k, v, mask=None):\n",
    "  qk = q @ k.transpose(-1, -2) / (k.shape[-1] ** 0.5)\n",
    "  if mask is not None:\n",
    "    qk = qk + mask\n",
    "  weights = torch.softmax(qk, dim=-1)\n",
    "  return weights @ v\n",
    "\n",
    "# (batch, seq, feature)\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "  def __init__(self, embed_size, heads, max_length):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_size = embed_size // heads\n",
    "    self.max_length = max_length\n",
    "\n",
    "    self.mask = torch.zeros((max_length, max_length))\n",
    "    ix = torch.triu_indices(max_length, max_length, 1)\n",
    "    self.mask[ix[0], ix[1]] = -1e9\n",
    "\n",
    "    self.Wq = torch.nn.Linear(embed_size, embed_size)\n",
    "    self.Wk = torch.nn.Linear(embed_size, embed_size)\n",
    "    self.Wv = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    self.Wo = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, s, e = x.shape\n",
    "    self.mask = self.mask.to(x.device)\n",
    "    # x is (batch, seq_len, embed_size)\n",
    "    q = self.Wq(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    k = self.Wk(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    v = self.Wv(x).view(b, s, self.heads, self.head_size).transpose(1, 2).contiguous()\n",
    "    x = attention(q, k, v, self.mask)\n",
    "    # x is now (heads, seq_len, head_size)\n",
    "    x = x.transpose(1, 2).contiguous().view(b, s, self.embed_size)\n",
    "    return self.Wo(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 8]), 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the MultiHeadAttention\n",
    "embed_size = 8\n",
    "heads = 4\n",
    "x = torch.randn(2, 10, embed_size)\n",
    "mha = MultiHeadAttention(embed_size, heads, 10)\n",
    "mha(x)\n",
    "x.shape, x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha 288\n",
      "mha2 288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 8]), torch.Size([1, 10, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 8\n",
    "heads = 2\n",
    "x = torch.randn(1, 10, embed_size)\n",
    "\n",
    "mha = torch.nn.MultiheadAttention(embed_size, heads, batch_first=True)\n",
    "mha_output, _ = mha(x, x, x, need_weights=False)\n",
    "\n",
    "print('mha', sum([p.nelement() for p in mha.parameters()]))\n",
    "\n",
    "mha2 = MultiHeadAttention(embed_size, heads, 10)\n",
    "print('mha2', sum([p.nelement() for p in mha2.parameters()]))\n",
    "\n",
    "mha2_output = mha2(x)\n",
    "\n",
    "mha_output.shape, mha2_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq, feature)\n",
    "class FeedForward(torch.nn.Module):\n",
    "  def __init__(self, embed_size):\n",
    "    super(FeedForward, self).__init__()\n",
    "    self.main = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embed_size, embed_size * 4),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(embed_size * 4, embed_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "  def __init__(self, embed_size, heads, max_length):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "\n",
    "    self.attention = MultiHeadAttention(embed_size, heads, max_length)\n",
    "    self.norm1 = torch.nn.LayerNorm(embed_size)\n",
    "    self.norm2 = torch.nn.LayerNorm(embed_size)\n",
    "    self.ff = FeedForward(embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    attended = self.attention(x)\n",
    "    x = self.norm1(attended + x)\n",
    "    fed = self.ff(x)\n",
    "    x = self.norm2(fed + x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batches first\n",
    "class LLM(torch.nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, depth, heads, max_length):\n",
    "    super(LLM, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.max_length = max_length\n",
    "    self.depth = depth\n",
    "    self.heads = heads\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_enc = PositionEncoding(max_length, embed_size)\n",
    "    self.decoder = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    blocks = [TransformerBlock(embed_size, heads, max_length) for _ in range(depth)]\n",
    "    self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    # position encoding\n",
    "    x = self.pos_enc(x)\n",
    "    # feed through transformer blocks\n",
    "    x = self.blocks(x)\n",
    "    out = self.decoder(x)\n",
    "    return out # torch.softmax(out, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "embed_size = 128\n",
    "depth = 10\n",
    "heads = 4\n",
    "max_length = 256\n",
    "batch_size = 32\n",
    "model = LLM(vocab_size, embed_size, depth, heads, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.tensor(encoded[:max_length]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196571"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of model\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def generate(model, encoded_prompt, length):\n",
    "#  model.eval()\n",
    "  x = torch.tensor(encoded_prompt).unsqueeze(0).to('cuda')\n",
    "  with torch.no_grad():\n",
    "    for i in range(length):\n",
    "      y = model(x[0, -max_length:])\n",
    "      y = torch.softmax(y, dim=-1)\n",
    "      y = torch.multinomial(y[0, -1, :], 1).unsqueeze(0)\n",
    "      x = torch.cat([x, y], dim=1)\n",
    "  return ''.join([chars[i] for i in x[0]])\n",
    "\n",
    "def train(model, optim, loss_fn, data, epochs=10, device=\"cpu\"):\n",
    "  model.to(device)\n",
    "  lossi = []\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(len(data) // max_length // batch_size):\n",
    "      ix = torch.randint(0, len(data) - max_length - 1, (batch_size,))\n",
    "      x = torch.tensor([data[i:i + max_length] for i in ix]).to(device)\n",
    "      y = torch.tensor([data[i + 1:i + max_length + 1] for i in ix]).to(device)\n",
    "      if y.shape[1] != max_length:\n",
    "        continue\n",
    "      y_hat = model(x)\n",
    "      loss = loss_fn(y_hat.view(-1, y_hat.shape[-1]), y.view(-1))\n",
    "      lossi.append(loss.item())\n",
    "      optim.zero_grad()\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "      if len(lossi)%1000 == 0:\n",
    "        print(f\"epoch {epoch} i {i} loss {sum(lossi)/len(lossi)}\")\n",
    "        lossi = []\n",
    "    output = generate(model, encoded[:max_length], 100)\n",
    "    output = output[-100:]\n",
    "    print(f\"epoch {epoch} loss {loss.item()}: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 i 999 loss 2.509027638435364\n",
      "epoch 0 i 1999 loss 2.394892918109894\n",
      "epoch 0 i 2999 loss 2.374404799461365\n",
      "epoch 0 i 3999 loss 2.357421378612518\n",
      "epoch 0 i 4999 loss 2.338097105026245\n",
      "epoch 0 i 5999 loss 2.3154317591190337\n",
      "epoch 0 i 6999 loss 2.264230298757553\n",
      "epoch 0 i 7999 loss 2.205771980762482\n",
      "epoch 0 i 8999 loss 2.1559656829833984\n",
      "epoch 0 i 9999 loss 2.111240647315979\n",
      "epoch 0 i 10999 loss 2.072074129462242\n",
      "epoch 0 i 11999 loss 2.039836578965187\n",
      "epoch 0 loss 1.9761521816253662: res orat ereklvent of hire adechy ins gecont atto wo mas thaco thin ito thros the wat thece the deth\n",
      "epoch 1 i 792 loss 2.0140576425790786\n",
      "epoch 1 i 1792 loss 1.9913155736923218\n",
      "epoch 1 i 2792 loss 1.9703298182487488\n",
      "epoch 1 i 3792 loss 1.9519757535457611\n",
      "epoch 1 i 4792 loss 1.9351621257066727\n",
      "epoch 1 i 5792 loss 1.9194907071590424\n",
      "epoch 1 i 6792 loss 1.90385358273983\n",
      "epoch 1 i 7792 loss 1.8881795470714569\n",
      "epoch 1 i 8792 loss 1.8735385894775392\n",
      "epoch 1 i 9792 loss 1.8583231242895126\n",
      "epoch 1 i 10792 loss 1.8457638157606124\n",
      "epoch 1 i 11792 loss 1.8361928426027299\n",
      "epoch 1 loss 1.8183666467666626: e livibes of four three the fiest afsiriaten yoush is comoul ocks enourar they colong ppurostiouts a\n",
      "epoch 2 i 585 loss 1.8242472349405288\n",
      "epoch 2 i 1585 loss 1.8128220899105072\n",
      "epoch 2 i 2585 loss 1.8026871973276137\n",
      "epoch 2 i 3585 loss 1.79327274787426\n",
      "epoch 2 i 4585 loss 1.7824881390333176\n",
      "epoch 2 i 5585 loss 1.774807132601738\n",
      "epoch 2 i 6585 loss 1.7640998668670653\n",
      "epoch 2 i 7585 loss 1.7558479310274124\n",
      "epoch 2 i 8585 loss 1.7495709688663483\n",
      "epoch 2 i 9585 loss 1.741837594628334\n",
      "epoch 2 i 10585 loss 1.7337677381038665\n",
      "epoch 2 i 11585 loss 1.7254172953367233\n",
      "epoch 2 loss 1.7233946323394775: ree pre dond vary coppreled is ratern madinition it one five sides groost actool vegry aguble the pr\n",
      "epoch 3 i 378 loss 1.7202277092933655\n",
      "epoch 3 i 1378 loss 1.7136987029314041\n",
      "epoch 3 i 2378 loss 1.7063044624328614\n",
      "epoch 3 i 3378 loss 1.7018005760908126\n",
      "epoch 3 i 4378 loss 1.6929952087402345\n",
      "epoch 3 i 5378 loss 1.6875213391780852\n",
      "epoch 3 i 6378 loss 1.6831421937942506\n",
      "epoch 3 i 7378 loss 1.6770082569122315\n",
      "epoch 3 i 8378 loss 1.6720561249256134\n",
      "epoch 3 i 9378 loss 1.667451971411705\n",
      "epoch 3 i 10378 loss 1.6636540536880493\n",
      "epoch 3 i 11378 loss 1.6572694116830826\n",
      "epoch 3 loss 1.6759089231491089: e ucccopols terublition the bengleshic powers year the trough polical gainsmic parber beling and uky\n",
      "epoch 4 i 171 loss 1.652457473397255\n",
      "epoch 4 i 1171 loss 1.6444312509298324\n",
      "epoch 4 i 2171 loss 1.6450153467655182\n",
      "epoch 4 i 3171 loss 1.6391054545640946\n",
      "epoch 4 i 4171 loss 1.636773470878601\n",
      "epoch 4 i 5171 loss 1.631157365679741\n",
      "epoch 4 i 6171 loss 1.625207795023918\n",
      "epoch 4 i 7171 loss 1.6225686801671981\n",
      "epoch 4 i 8171 loss 1.6196745138168336\n",
      "epoch 4 i 9171 loss 1.6163544704914092\n",
      "epoch 4 i 10171 loss 1.612642635822296\n",
      "epoch 4 i 11171 loss 1.6091740701198578\n",
      "epoch 4 i 12171 loss 1.60750517141819\n",
      "epoch 4 loss 1.5435842275619507: en alson as but to court senveral lapp that essequestived whish feark kild arrand the just defince f\n",
      "epoch 5 i 964 loss 1.6008513473272323\n",
      "epoch 5 i 1964 loss 1.5997284539937973\n",
      "epoch 5 i 2964 loss 1.5960352114439011\n",
      "epoch 5 i 3964 loss 1.5928615568876265\n",
      "epoch 5 i 4964 loss 1.5915080797672272\n",
      "epoch 5 i 5964 loss 1.5894390614032745\n",
      "epoch 5 i 6964 loss 1.5862289569377899\n",
      "epoch 5 i 7964 loss 1.583062662243843\n",
      "epoch 5 i 8964 loss 1.5794562097787856\n",
      "epoch 5 i 9964 loss 1.5768339977264405\n",
      "epoch 5 i 10964 loss 1.5714108253717423\n",
      "epoch 5 i 11964 loss 1.5700929681062699\n",
      "epoch 5 loss 1.5477131605148315: e the tiecly of the end mast wach were as whee meake lendent because and lead reaced and the two pir\n",
      "epoch 6 i 757 loss 1.5684697355031967\n",
      "epoch 6 i 1757 loss 1.5660392825603484\n",
      "epoch 6 i 2757 loss 1.5597301313877105\n",
      "epoch 6 i 3757 loss 1.5604413678646087\n",
      "epoch 6 i 4757 loss 1.5578662989139558\n",
      "epoch 6 i 5757 loss 1.5564147386550904\n",
      "epoch 6 i 6757 loss 1.5548000341653825\n",
      "epoch 6 i 7757 loss 1.5518164539337158\n",
      "epoch 6 i 8757 loss 1.5498864353895188\n",
      "epoch 6 i 9757 loss 1.545552157998085\n",
      "epoch 6 i 10757 loss 1.5441774401664734\n",
      "epoch 6 i 11757 loss 1.5426965630054474\n",
      "epoch 6 loss 1.596156358718872: is hendlow historist were as fremed interh create experts and have text to the hortiere old were sig\n",
      "epoch 7 i 550 loss 1.5396896432638167\n",
      "epoch 7 i 1550 loss 1.5383812642097474\n",
      "epoch 7 i 2550 loss 1.5373989865779876\n",
      "epoch 7 i 3550 loss 1.5362532421350479\n",
      "epoch 7 i 4550 loss 1.5329795147180558\n",
      "epoch 7 i 5550 loss 1.5331646461486816\n",
      "epoch 7 i 6550 loss 1.5294205816984177\n",
      "epoch 7 i 7550 loss 1.5311784732341767\n",
      "epoch 7 i 8550 loss 1.5254892033338547\n",
      "epoch 7 i 9550 loss 1.5231419804096222\n",
      "epoch 7 i 10550 loss 1.5207703592777253\n",
      "epoch 7 i 11550 loss 1.5200538265705108\n",
      "epoch 7 loss 1.5470422506332397: e show clusting the two since percentant mirco hey laters west some seven histrent this assund can o\n",
      "epoch 8 i 343 loss 1.5198188030719757\n",
      "epoch 8 i 1343 loss 1.5179850845336913\n",
      "epoch 8 i 2343 loss 1.5152123906612396\n",
      "epoch 8 i 3343 loss 1.5174593135118484\n",
      "epoch 8 i 4343 loss 1.5120638320446014\n",
      "epoch 8 i 5343 loss 1.5091611477136613\n",
      "epoch 8 i 6343 loss 1.5097576389312743\n",
      "epoch 8 i 7343 loss 1.5048407963514328\n",
      "epoch 8 i 8343 loss 1.5075433396101\n",
      "epoch 8 i 9343 loss 1.5056042163372039\n",
      "epoch 8 i 10343 loss 1.504255187034607\n",
      "epoch 8 i 11343 loss 1.5015460240840912\n",
      "epoch 8 loss 1.4745702743530273: e note textwoders from q assol the kid range for it terropeists underriguttion would paceoetned to t\n",
      "epoch 9 i 136 loss 1.5025804224014283\n",
      "epoch 9 i 1136 loss 1.4987767624855042\n",
      "epoch 9 i 2136 loss 1.4997886089086532\n",
      "epoch 9 i 3136 loss 1.497664826154709\n",
      "epoch 9 i 4136 loss 1.4976543558835984\n",
      "epoch 9 i 5136 loss 1.4946267298460008\n",
      "epoch 9 i 6136 loss 1.4928488241434097\n",
      "epoch 9 i 7136 loss 1.4922704582214354\n",
      "epoch 9 i 8136 loss 1.4874667257070542\n",
      "epoch 9 i 9136 loss 1.4894850685596466\n",
      "epoch 9 i 10136 loss 1.4889289704561233\n",
      "epoch 9 i 11136 loss 1.4858205921649932\n",
      "epoch 9 i 12136 loss 1.4846309328079224\n",
      "epoch 9 loss 1.4994598627090454: e presised from of harneford and historyn see aways seudo in wheatern fan all aimed to the nine sebv\n",
      "epoch 10 i 929 loss 1.4832553869485856\n",
      "epoch 10 i 1929 loss 1.482054314494133\n",
      "epoch 10 i 2929 loss 1.4815865442752838\n",
      "epoch 10 i 3929 loss 1.4785444922447204\n",
      "epoch 10 i 4929 loss 1.4799455600976943\n",
      "epoch 10 i 5929 loss 1.477406689286232\n",
      "epoch 10 i 6929 loss 1.4773263728618622\n",
      "epoch 10 i 7929 loss 1.474666590809822\n",
      "epoch 10 i 8929 loss 1.4770214015245438\n",
      "epoch 10 i 9929 loss 1.4728841453790664\n",
      "epoch 10 i 10929 loss 1.472725979089737\n",
      "epoch 10 i 11929 loss 1.4715663087368012\n",
      "epoch 10 loss 1.456847906112671: e family beartically sacraft only iski water gradinkh american pliesto stepted in people experies of\n",
      "epoch 11 i 722 loss 1.4709725737571717\n",
      "epoch 11 i 1722 loss 1.471445853948593\n",
      "epoch 11 i 2722 loss 1.46891559779644\n",
      "epoch 11 i 3722 loss 1.4707867861986161\n",
      "epoch 11 i 4722 loss 1.4678003352880478\n",
      "epoch 11 i 5722 loss 1.4671314767599106\n",
      "epoch 11 i 6722 loss 1.4656665638685227\n",
      "epoch 11 i 7722 loss 1.4643318630456925\n",
      "epoch 11 i 8722 loss 1.4644929928779602\n",
      "epoch 11 i 9722 loss 1.4602054870128631\n",
      "epoch 11 i 10722 loss 1.4601363731622696\n",
      "epoch 11 i 11722 loss 1.4586194690465928\n",
      "epoch 11 loss 1.4671761989593506: e electation but some discumender from an ancestack conferry with momonly especion and the one nine \n",
      "epoch 12 i 515 loss 1.4616152577400208\n",
      "epoch 12 i 1515 loss 1.459771438241005\n",
      "epoch 12 i 2515 loss 1.4583959387540817\n",
      "epoch 12 i 3515 loss 1.4572890825271607\n",
      "epoch 12 i 4515 loss 1.4563316628932952\n",
      "epoch 12 i 5515 loss 1.4540215190649033\n",
      "epoch 12 i 6515 loss 1.456045096874237\n",
      "epoch 12 i 7515 loss 1.4514573085308076\n",
      "epoch 12 i 8515 loss 1.4552808191776276\n",
      "epoch 12 i 9515 loss 1.4514638092517853\n",
      "epoch 12 i 10515 loss 1.4504639528989791\n",
      "epoch 12 i 11515 loss 1.450014604449272\n",
      "epoch 12 loss 1.461709976196289: e the remored that raw in western significant with the emiging s involving out of the sun cade argum\n",
      "epoch 13 i 308 loss 1.4493558350801468\n",
      "epoch 13 i 1308 loss 1.448170419216156\n",
      "epoch 13 i 2308 loss 1.447275146007538\n",
      "epoch 13 i 3308 loss 1.4448714301586152\n",
      "epoch 13 i 4308 loss 1.4465070852041244\n",
      "epoch 13 i 5308 loss 1.4460199270248413\n",
      "epoch 13 i 6308 loss 1.4445825281143188\n",
      "epoch 13 i 7308 loss 1.442831377863884\n",
      "epoch 13 i 8308 loss 1.4430359278917313\n",
      "epoch 13 i 9308 loss 1.441037393808365\n",
      "epoch 13 i 10308 loss 1.4408089984655381\n",
      "epoch 13 i 11308 loss 1.4412946363687515\n",
      "epoch 13 loss 1.4334607124328613: e extensive malay an general country book whom a servicely stes of resulterism of the taball enga ot\n",
      "epoch 14 i 101 loss 1.4368942630290986\n",
      "epoch 14 i 1101 loss 1.4383351460695266\n",
      "epoch 14 i 2101 loss 1.4374740279912948\n",
      "epoch 14 i 3101 loss 1.4363474749326706\n",
      "epoch 14 i 4101 loss 1.4361235113143922\n",
      "epoch 14 i 5101 loss 1.4359904005527497\n",
      "epoch 14 i 6101 loss 1.4343837624788285\n",
      "epoch 14 i 7101 loss 1.4337697805166245\n",
      "epoch 14 i 8101 loss 1.4339424554109574\n",
      "epoch 14 i 9101 loss 1.4308874059915542\n",
      "epoch 14 i 10101 loss 1.433796905875206\n",
      "epoch 14 i 11101 loss 1.4303828188180923\n",
      "epoch 14 i 12101 loss 1.4303719092607499\n",
      "epoch 14 loss 1.4124798774719238: e term of highly itself assolined minority clancing of history not a year the sextsive of caise on h\n",
      "epoch 15 i 894 loss 1.4317567912340163\n",
      "epoch 15 i 1894 loss 1.4262599472999573\n",
      "epoch 15 i 2894 loss 1.4278444371223449\n",
      "epoch 15 i 3894 loss 1.4288142775297166\n",
      "epoch 15 i 4894 loss 1.4284990218877793\n",
      "epoch 15 i 5894 loss 1.424414252281189\n",
      "epoch 15 i 6894 loss 1.4262543489933015\n",
      "epoch 15 i 7894 loss 1.428703413963318\n",
      "epoch 15 i 8894 loss 1.4267426471710205\n",
      "epoch 15 i 9894 loss 1.4228800840377807\n",
      "epoch 15 i 10894 loss 1.4234691545963287\n",
      "epoch 15 i 11894 loss 1.4239841611385347\n",
      "epoch 15 loss 1.4528051614761353: e lew in island in food the state image system mendicated on arnow scholatics whose started is germa\n",
      "epoch 16 i 687 loss 1.4233208339214325\n",
      "epoch 16 i 1687 loss 1.4219213958978654\n",
      "epoch 16 i 2687 loss 1.4210828679800034\n",
      "epoch 16 i 3687 loss 1.419219319820404\n",
      "epoch 16 i 4687 loss 1.419825789809227\n",
      "epoch 16 i 5687 loss 1.4195198260545732\n",
      "epoch 16 i 6687 loss 1.42043608045578\n",
      "epoch 16 i 7687 loss 1.4195311094522476\n",
      "epoch 16 i 8687 loss 1.4179672828912735\n",
      "epoch 16 i 9687 loss 1.416807835817337\n",
      "epoch 16 i 10687 loss 1.4170134223699569\n",
      "epoch 16 i 11687 loss 1.4157933291196823\n",
      "epoch 16 loss 1.4470627307891846: e rebelent europements to drugg for offity a centre writing s and the english singers dc s an eyes o\n",
      "epoch 17 i 480 loss 1.4145650504827498\n",
      "epoch 17 i 1480 loss 1.4137549772262574\n",
      "epoch 17 i 2480 loss 1.412466678261757\n",
      "epoch 17 i 3480 loss 1.4140092787742615\n",
      "epoch 17 i 4480 loss 1.4133897780179978\n",
      "epoch 17 i 5480 loss 1.4129494224786758\n",
      "epoch 17 i 6480 loss 1.4113823932409286\n",
      "epoch 17 i 7480 loss 1.4101838129758835\n",
      "epoch 17 i 8480 loss 1.4143202790021896\n",
      "epoch 17 i 9480 loss 1.4081052223443984\n",
      "epoch 17 i 10480 loss 1.4081233271360398\n",
      "epoch 17 i 11480 loss 1.40961816573143\n",
      "epoch 17 loss 1.326701045036316: e tiests properly the large lent analysis tower century of farity the bread the spot lion stations s\n",
      "epoch 18 i 273 loss 1.4092451448440553\n",
      "epoch 18 i 1273 loss 1.4086440744400024\n",
      "epoch 18 i 2273 loss 1.4063042423725127\n",
      "epoch 18 i 3273 loss 1.4066531845331192\n",
      "epoch 18 i 4273 loss 1.4066497080326081\n",
      "epoch 18 i 5273 loss 1.4067994649410247\n",
      "epoch 18 i 6273 loss 1.4059391137361525\n",
      "epoch 18 i 7273 loss 1.4051090531349182\n",
      "epoch 18 i 8273 loss 1.4051999971866607\n",
      "epoch 18 i 9273 loss 1.4043552124500274\n",
      "epoch 18 i 10273 loss 1.4045928344726561\n",
      "epoch 18 i 11273 loss 1.4037228286266328\n",
      "epoch 18 loss 1.3382527828216553: e adagned by peter by denputually reased a tran rail folked battle during there lating of the appeal\n",
      "epoch 19 i 66 loss 1.401929467201233\n",
      "epoch 19 i 1066 loss 1.4026105709075927\n",
      "epoch 19 i 2066 loss 1.402137838602066\n",
      "epoch 19 i 3066 loss 1.3999577786922455\n",
      "epoch 19 i 4066 loss 1.4001488831043243\n",
      "epoch 19 i 5066 loss 1.3995715129375457\n",
      "epoch 19 i 6066 loss 1.4002362602949143\n",
      "epoch 19 i 7066 loss 1.4005476438999176\n",
      "epoch 19 i 8066 loss 1.3997574011087417\n",
      "epoch 19 i 9066 loss 1.3968093571662903\n",
      "epoch 19 i 10066 loss 1.3989052628278733\n",
      "epoch 19 i 11066 loss 1.3990823961496353\n",
      "epoch 19 i 12066 loss 1.3970467338562012\n",
      "epoch 19 loss 1.3799561262130737: em have of requirest of dictional occurs order a far the control attiments or lite x low first land \n",
      "epoch 20 i 859 loss 1.395907147526741\n",
      "epoch 20 i 1859 loss 1.3960263814926148\n",
      "epoch 20 i 2859 loss 1.3961397808790208\n",
      "epoch 20 i 3859 loss 1.3955299664735794\n",
      "epoch 20 i 4859 loss 1.3950886853933335\n",
      "epoch 20 i 5859 loss 1.3942133792638778\n",
      "epoch 20 i 6859 loss 1.3920130013227463\n",
      "epoch 20 i 7859 loss 1.3954891657829285\n",
      "epoch 20 i 8859 loss 1.3940024769306183\n",
      "epoch 20 i 9859 loss 1.3935908079147339\n",
      "epoch 20 i 10859 loss 1.3903675312995911\n",
      "epoch 20 i 11859 loss 1.3916405948400496\n",
      "epoch 20 loss 1.3400989770889282: eir circlate of death country such assumetists oriented states vaichima the very prancipate philosop\n",
      "epoch 21 i 652 loss 1.3908188531398773\n",
      "epoch 21 i 1652 loss 1.3899427375793456\n",
      "epoch 21 i 2652 loss 1.3911531178951264\n",
      "epoch 21 i 3652 loss 1.387839556336403\n",
      "epoch 21 i 4652 loss 1.3893357248306275\n",
      "epoch 21 i 5652 loss 1.3883052963018416\n",
      "epoch 21 i 6652 loss 1.3885826365947724\n",
      "epoch 21 i 7652 loss 1.3878842313289643\n",
      "epoch 21 i 8652 loss 1.3864605141878128\n",
      "epoch 21 i 9652 loss 1.3854620550870895\n",
      "epoch 21 i 10652 loss 1.3870726021528244\n",
      "epoch 21 i 11652 loss 1.3868596658706664\n",
      "epoch 21 loss 1.3833779096603394: e centre of a teching eithtenent is dialecting currently between as a curved cash one six neight cou\n",
      "epoch 22 i 445 loss 1.3848662536144256\n",
      "epoch 22 i 1445 loss 1.3869746794700624\n",
      "epoch 22 i 2445 loss 1.3854342629909515\n",
      "epoch 22 i 3445 loss 1.3865241327285767\n",
      "epoch 22 i 4445 loss 1.3821474828720093\n",
      "epoch 22 i 5445 loss 1.3832453566789626\n",
      "epoch 22 i 6445 loss 1.3832605063915253\n",
      "epoch 22 i 7445 loss 1.3811674659252167\n",
      "epoch 22 i 8445 loss 1.3820209795236587\n",
      "epoch 22 i 9445 loss 1.381951247215271\n",
      "epoch 22 i 10445 loss 1.382236527323723\n",
      "epoch 22 i 11445 loss 1.3817127795219422\n",
      "epoch 22 loss 1.360331654548645: ought are the first than it the scould increased to that many daysin continued panal letter these de\n",
      "epoch 23 i 238 loss 1.3827507708072662\n",
      "epoch 23 i 1238 loss 1.3823910359144211\n",
      "epoch 23 i 2238 loss 1.378651166677475\n",
      "epoch 23 i 3238 loss 1.3791491215229035\n",
      "epoch 23 i 4238 loss 1.3796885690689087\n",
      "epoch 23 i 5238 loss 1.3791313365697861\n",
      "epoch 23 i 6238 loss 1.3778595675230025\n",
      "epoch 23 i 7238 loss 1.3790064878463746\n",
      "epoch 23 i 8238 loss 1.3774078335762023\n",
      "epoch 23 i 9238 loss 1.3774125703573228\n",
      "epoch 23 i 10238 loss 1.3775692735910416\n",
      "epoch 23 i 11238 loss 1.375985852956772\n",
      "epoch 23 loss 1.4387956857681274: e west in one one six supplyidely naturedisant protel exist modit some sookolmatic after the freeds \n",
      "epoch 24 i 31 loss 1.3770487133264542\n",
      "epoch 24 i 1031 loss 1.3774725453853607\n",
      "epoch 24 i 2031 loss 1.3749152079820632\n",
      "epoch 24 i 3031 loss 1.3759651064872742\n",
      "epoch 24 i 4031 loss 1.3726388323307037\n",
      "epoch 24 i 5031 loss 1.3742452627420425\n",
      "epoch 24 i 6031 loss 1.3759065647125244\n",
      "epoch 24 i 7031 loss 1.3739682422876358\n",
      "epoch 24 i 8031 loss 1.3736134890317917\n",
      "epoch 24 i 9031 loss 1.3715027933120727\n",
      "epoch 24 i 10031 loss 1.371613558769226\n",
      "epoch 24 i 11031 loss 1.3720915058851242\n",
      "epoch 24 i 12031 loss 1.3733447630405426\n",
      "epoch 24 loss 1.397477626800537: eory to overhed in the telecomments from thrones in adam eternal sentence was romany while hell as w\n",
      "epoch 25 i 824 loss 1.370840866446495\n",
      "epoch 25 i 1824 loss 1.3717879391908645\n",
      "epoch 25 i 2824 loss 1.3732880923748016\n",
      "epoch 25 i 3824 loss 1.370269153356552\n",
      "epoch 25 i 4824 loss 1.369297222137451\n",
      "epoch 25 i 5824 loss 1.369779762029648\n",
      "epoch 25 i 6824 loss 1.3693637491464614\n",
      "epoch 25 i 7824 loss 1.3674112119674682\n",
      "epoch 25 i 8824 loss 1.3687360228300094\n",
      "epoch 25 i 9824 loss 1.3673015466928482\n",
      "epoch 25 i 10824 loss 1.3662771937847138\n",
      "epoch 25 i 11824 loss 1.3701467937231064\n",
      "epoch 25 loss 1.3279616832733154: e mongost life as an even sometimes are author enzer game is also a situal diac pronut sum east docu\n",
      "epoch 26 i 617 loss 1.3685126893520356\n",
      "epoch 26 i 1617 loss 1.3667906044721603\n",
      "epoch 26 i 2617 loss 1.3643093693256378\n",
      "epoch 26 i 3617 loss 1.3642202882766723\n",
      "epoch 26 i 4617 loss 1.3683328772783279\n",
      "epoch 26 i 5617 loss 1.3667910552024842\n",
      "epoch 26 i 6617 loss 1.3664772444963456\n",
      "epoch 26 i 7617 loss 1.3642736546993255\n",
      "epoch 26 i 8617 loss 1.3648564275503159\n",
      "epoch 26 i 9617 loss 1.364344695329666\n",
      "epoch 26 i 10617 loss 1.36366919028759\n",
      "epoch 26 i 11617 loss 1.3632285999059677\n",
      "epoch 26 loss 1.3921194076538086: ree years open wahel time squair venesels and little tend on nucles by gold automatically conded by \n",
      "epoch 27 i 410 loss 1.3626889860630036\n",
      "epoch 27 i 1410 loss 1.3643718312978745\n",
      "epoch 27 i 2410 loss 1.3639812333583832\n",
      "epoch 27 i 3410 loss 1.3636143954992295\n",
      "epoch 27 i 4410 loss 1.3621168802976609\n",
      "epoch 27 i 5410 loss 1.363841769695282\n",
      "epoch 27 i 6410 loss 1.360950397014618\n",
      "epoch 27 i 7410 loss 1.35697711122036\n",
      "epoch 27 i 8410 loss 1.3616800562143325\n",
      "epoch 27 i 9410 loss 1.362268840074539\n",
      "epoch 27 i 10410 loss 1.3593632028102876\n",
      "epoch 27 i 11410 loss 1.3589647005796432\n",
      "epoch 27 loss 1.4099634885787964: e face was must of orbittion to one at that a this games rich expressive that people of is severe a \n",
      "epoch 28 i 203 loss 1.3583807396888732\n",
      "epoch 28 i 1203 loss 1.3601089621782303\n",
      "epoch 28 i 2203 loss 1.3580384021997451\n",
      "epoch 28 i 3203 loss 1.359313428759575\n",
      "epoch 28 i 4203 loss 1.3587196053266526\n",
      "epoch 28 i 5203 loss 1.3580608319044114\n",
      "epoch 28 i 6203 loss 1.3551228930950165\n",
      "epoch 28 i 7203 loss 1.3595380679368974\n",
      "epoch 28 i 8203 loss 1.3548623176813126\n",
      "epoch 28 i 9203 loss 1.3571084614992142\n",
      "epoch 28 i 10203 loss 1.3542863976955415\n",
      "epoch 28 i 11203 loss 1.3558273783922195\n",
      "epoch 28 i 12203 loss 1.35397201859951\n",
      "epoch 28 loss 1.296144962310791: e branchess to wede agrams instructions questions over group e usually were in the one nine zero in \n",
      "epoch 29 i 996 loss 1.3547365267276763\n",
      "epoch 29 i 1996 loss 1.354445356607437\n",
      "epoch 29 i 2996 loss 1.3531605541706084\n",
      "epoch 29 i 3996 loss 1.353164006471634\n",
      "epoch 29 i 4996 loss 1.3518499063253402\n",
      "epoch 29 i 5996 loss 1.3540965924263\n",
      "epoch 29 i 6996 loss 1.3527656419277192\n",
      "epoch 29 i 7996 loss 1.352537902712822\n",
      "epoch 29 i 8996 loss 1.3527720357179642\n",
      "epoch 29 i 9996 loss 1.3510474153757095\n",
      "epoch 29 i 10996 loss 1.3496597946882247\n",
      "epoch 29 i 11996 loss 1.3504368575811385\n",
      "epoch 29 loss 1.3593549728393555: e northern treated with class the large to three build town a fertilato in the next state general la\n",
      "epoch 30 i 789 loss 1.3517818043231964\n",
      "epoch 30 i 1789 loss 1.3489250791072847\n",
      "epoch 30 i 2789 loss 1.349435503721237\n",
      "epoch 30 i 3789 loss 1.348879334449768\n",
      "epoch 30 i 4789 loss 1.3509288058280944\n",
      "epoch 30 i 5789 loss 1.3502506844997406\n",
      "epoch 30 i 6789 loss 1.3489434099197388\n",
      "epoch 30 i 7789 loss 1.346472684621811\n",
      "epoch 30 i 8789 loss 1.348682965159416\n",
      "epoch 30 i 9789 loss 1.3492467252016067\n",
      "epoch 30 i 10789 loss 1.3486193449497224\n",
      "epoch 30 i 11789 loss 1.3468006511926651\n",
      "epoch 30 loss 1.3431442975997925: e point area s slighter that woult contradge a father difficult of the average climinate difference \n",
      "epoch 31 i 582 loss 1.3461747885942459\n",
      "epoch 31 i 1582 loss 1.345489408135414\n",
      "epoch 31 i 2582 loss 1.3444750807285308\n",
      "epoch 31 i 3582 loss 1.344972195148468\n",
      "epoch 31 i 4582 loss 1.3459806525707245\n",
      "epoch 31 i 5582 loss 1.3436134557724\n",
      "epoch 31 i 6582 loss 1.3443592450618744\n",
      "epoch 31 i 7582 loss 1.3450858628749847\n",
      "epoch 31 i 8582 loss 1.3439288231134414\n",
      "epoch 31 i 9582 loss 1.3446427444219589\n",
      "epoch 31 i 10582 loss 1.3452979884147644\n",
      "epoch 31 i 11582 loss 1.3433839271068573\n",
      "epoch 31 loss 1.3062419891357422: e one timelly west approached may immediated by theodography leaped in fem the paradiate engine refe\n",
      "epoch 32 i 375 loss 1.3423962850570679\n",
      "epoch 32 i 1375 loss 1.3422736159563065\n",
      "epoch 32 i 2375 loss 1.3417238159179687\n",
      "epoch 32 i 3375 loss 1.341277433514595\n",
      "epoch 32 i 4375 loss 1.3413829681873322\n",
      "epoch 32 i 5375 loss 1.3402915830612183\n",
      "epoch 32 i 6375 loss 1.3396833493709563\n",
      "epoch 32 i 7375 loss 1.33860351061821\n",
      "epoch 32 i 8375 loss 1.3396193141937256\n",
      "epoch 32 i 9375 loss 1.3378576263189317\n",
      "epoch 32 i 10375 loss 1.3380347057580948\n",
      "epoch 32 i 11375 loss 1.33987393784523\n",
      "epoch 32 loss 1.337016224861145: e discoveral subult europeas mccycree problem institing the island becomes who needed to known usual\n",
      "epoch 33 i 168 loss 1.3367091196775436\n",
      "epoch 33 i 1168 loss 1.3378328827619552\n",
      "epoch 33 i 2168 loss 1.335817105293274\n",
      "epoch 33 i 3168 loss 1.3358277447223663\n",
      "epoch 33 i 4168 loss 1.3359973829984666\n",
      "epoch 33 i 5168 loss 1.3345725178718566\n",
      "epoch 33 i 6168 loss 1.3352513500452041\n",
      "epoch 33 i 7168 loss 1.3356688029766082\n",
      "epoch 33 i 8168 loss 1.334214458823204\n",
      "epoch 33 i 9168 loss 1.3336583216190339\n",
      "epoch 33 i 10168 loss 1.3338711032867432\n",
      "epoch 33 i 11168 loss 1.3338765218257904\n",
      "epoch 33 i 12168 loss 1.3333621309995651\n",
      "epoch 33 loss 1.277557134628296: e treaty of norway to symbols officer london antipies in hebrews the bostor s a grosen of a colament\n",
      "epoch 34 i 961 loss 1.3325543646812439\n",
      "epoch 34 i 1961 loss 1.3343680682182313\n",
      "epoch 34 i 2961 loss 1.3326520917415619\n",
      "epoch 34 i 3961 loss 1.33048062479496\n",
      "epoch 34 i 4961 loss 1.332230880498886\n",
      "epoch 34 i 5961 loss 1.329835664153099\n",
      "epoch 34 i 6961 loss 1.3299848244190215\n",
      "epoch 34 i 7961 loss 1.329600005865097\n",
      "epoch 34 i 8961 loss 1.329559031844139\n",
      "epoch 34 i 9961 loss 1.3279634051322937\n",
      "epoch 34 i 10961 loss 1.3290984621047974\n",
      "epoch 34 i 11961 loss 1.3294560393095016\n",
      "epoch 34 loss 1.354196310043335: e incrediate image as the emulatoral apart the pullician diage or done the afc a c turn leve in the \n",
      "epoch 35 i 754 loss 1.3279810408353805\n",
      "epoch 35 i 1754 loss 1.328736343383789\n",
      "epoch 35 i 2754 loss 1.3294297692775727\n",
      "epoch 35 i 3754 loss 1.3277003543376922\n",
      "epoch 35 i 4754 loss 1.328613302707672\n",
      "epoch 35 i 5754 loss 1.3260395694971086\n",
      "epoch 35 i 6754 loss 1.325147084593773\n",
      "epoch 35 i 7754 loss 1.3251624690294266\n",
      "epoch 35 i 8754 loss 1.3248695673942565\n",
      "epoch 35 i 9754 loss 1.3254159904718399\n",
      "epoch 35 i 10754 loss 1.325907464504242\n",
      "epoch 35 i 11754 loss 1.326006116271019\n",
      "epoch 35 loss 1.290323257446289: e four in addiction procession japane execual s but not all of the sandhann see the culture is stock\n",
      "epoch 36 i 547 loss 1.3224612032175065\n",
      "epoch 36 i 1547 loss 1.3220555813312531\n",
      "epoch 36 i 2547 loss 1.3239418022632599\n",
      "epoch 36 i 3547 loss 1.3203744674921036\n",
      "epoch 36 i 4547 loss 1.321732572555542\n",
      "epoch 36 i 5547 loss 1.3195741750001908\n",
      "epoch 36 i 6547 loss 1.322639960050583\n",
      "epoch 36 i 7547 loss 1.3238395421504974\n",
      "epoch 36 i 8547 loss 1.3223975546360016\n",
      "epoch 36 i 9547 loss 1.3193826565742492\n",
      "epoch 36 i 10547 loss 1.318481478214264\n",
      "epoch 36 i 11547 loss 1.3195140707492827\n",
      "epoch 36 loss 1.284130573272705: e seignity every the use use arms list the argument a month armist is not affict only intisce the ea\n",
      "epoch 37 i 340 loss 1.3184126012325288\n",
      "epoch 37 i 1340 loss 1.319135080575943\n",
      "epoch 37 i 2340 loss 1.317895679116249\n",
      "epoch 37 i 3340 loss 1.3178854434490204\n",
      "epoch 37 i 4340 loss 1.316902661561966\n",
      "epoch 37 i 5340 loss 1.317447813630104\n",
      "epoch 37 i 6340 loss 1.3159944747686385\n",
      "epoch 37 i 7340 loss 1.31795745241642\n",
      "epoch 37 i 8340 loss 1.3163917307853699\n",
      "epoch 37 i 9340 loss 1.316700612783432\n",
      "epoch 37 i 10340 loss 1.3172899547815322\n",
      "epoch 37 i 11340 loss 1.315172410607338\n",
      "epoch 37 loss 1.2501161098480225: e relation which the cleety of these jesuitary next just his gainizes ready and starringe and his re\n",
      "epoch 38 i 133 loss 1.3140268352031708\n",
      "epoch 38 i 1133 loss 1.3146160261631012\n",
      "epoch 38 i 2133 loss 1.312641994714737\n",
      "epoch 38 i 3133 loss 1.3137405029535294\n",
      "epoch 38 i 4133 loss 1.3163000682592392\n",
      "epoch 38 i 5133 loss 1.3119867678880692\n",
      "epoch 38 i 6133 loss 1.3142607872486114\n",
      "epoch 38 i 7133 loss 1.31245050740242\n",
      "epoch 38 i 8133 loss 1.3104642795324326\n",
      "epoch 38 i 9133 loss 1.3130817439556122\n",
      "epoch 38 i 10133 loss 1.3101709048748016\n",
      "epoch 38 i 11133 loss 1.3120613845586777\n",
      "epoch 38 i 12133 loss 1.3102843440771104\n",
      "epoch 38 loss 1.3257184028625488: e period some of clearly that he repaised in amal persile many of the clear is not numbers that do s\n",
      "epoch 39 i 926 loss 1.3104950844049454\n",
      "epoch 39 i 1926 loss 1.3121053434610366\n",
      "epoch 39 i 2926 loss 1.3111430522203444\n",
      "epoch 39 i 3926 loss 1.3109191380739211\n",
      "epoch 39 i 4926 loss 1.3104749699831009\n",
      "epoch 39 i 5926 loss 1.307937335729599\n",
      "epoch 39 i 6926 loss 1.3108062162399292\n",
      "epoch 39 i 7926 loss 1.3067998887300492\n",
      "epoch 39 i 8926 loss 1.3072759571075439\n",
      "epoch 39 i 9926 loss 1.308642483472824\n",
      "epoch 39 i 10926 loss 1.305501194357872\n",
      "epoch 39 i 11926 loss 1.3082242912054063\n",
      "epoch 39 loss 1.3237396478652954: e need papal early considered into the laps to a biose in order laptonic to keep with the rootano re\n",
      "epoch 40 i 719 loss 1.3077402342557907\n",
      "epoch 40 i 1719 loss 1.3072230911254883\n",
      "epoch 40 i 2719 loss 1.3067652050256728\n",
      "epoch 40 i 3719 loss 1.3059542117118836\n",
      "epoch 40 i 4719 loss 1.3065874408483504\n",
      "epoch 40 i 5719 loss 1.3039591649770736\n",
      "epoch 40 i 6719 loss 1.306512796163559\n",
      "epoch 40 i 7719 loss 1.3055944768190384\n",
      "epoch 40 i 8719 loss 1.305565463423729\n",
      "epoch 40 i 9719 loss 1.3025878353118896\n",
      "epoch 40 i 10719 loss 1.3031141109466553\n",
      "epoch 40 i 11719 loss 1.3016991305351258\n",
      "epoch 40 loss 1.329316258430481: e weight josened and plain but coprises in plains and secrible around romanized on the euphemi culti\n",
      "epoch 41 i 512 loss 1.3049111505746842\n",
      "epoch 41 i 1512 loss 1.3012736356258392\n",
      "epoch 41 i 2512 loss 1.3043416695594787\n",
      "epoch 41 i 3512 loss 1.302802914738655\n",
      "epoch 41 i 4512 loss 1.3014632638692856\n",
      "epoch 41 i 5512 loss 1.3022089930772782\n",
      "epoch 41 i 6512 loss 1.3017220870256423\n",
      "epoch 41 i 7512 loss 1.300780018568039\n",
      "epoch 41 i 8512 loss 1.3015462701320648\n",
      "epoch 41 i 9512 loss 1.3005292677879334\n",
      "epoch 41 i 10512 loss 1.3026054520606996\n",
      "epoch 41 i 11512 loss 1.29962495136261\n",
      "epoch 41 loss 1.3181158304214478: e olighth is also not supply who whether figure eight that the lord this of collispri with can jewis\n",
      "epoch 42 i 305 loss 1.296073100209236\n",
      "epoch 42 i 1305 loss 1.2985698053836823\n",
      "epoch 42 i 2305 loss 1.295748498082161\n",
      "epoch 42 i 3305 loss 1.297823987722397\n",
      "epoch 42 i 4305 loss 1.299371938228607\n",
      "epoch 42 i 5305 loss 1.2986185026168824\n",
      "epoch 42 i 6305 loss 1.2978292291164397\n",
      "epoch 42 i 7305 loss 1.2969001482725144\n",
      "epoch 42 i 8305 loss 1.293377435684204\n",
      "epoch 42 i 9305 loss 1.2963150749206542\n",
      "epoch 42 i 10305 loss 1.297966492652893\n",
      "epoch 42 i 11305 loss 1.2965341252088547\n",
      "epoch 42 loss 1.2698086500167847: e if three resented in still co clinical was eventually into still greekly a role the term had rena \n",
      "epoch 43 i 98 loss 1.2954123138189315\n",
      "epoch 43 i 1098 loss 1.2964974290132523\n",
      "epoch 43 i 2098 loss 1.2935936170816422\n",
      "epoch 43 i 3098 loss 1.2952160704135895\n",
      "epoch 43 i 4098 loss 1.2927871501445771\n",
      "epoch 43 i 5098 loss 1.293735471010208\n",
      "epoch 43 i 6098 loss 1.294625920534134\n",
      "epoch 43 i 7098 loss 1.29494198179245\n",
      "epoch 43 i 8098 loss 1.2942871710062027\n",
      "epoch 43 i 9098 loss 1.2914381663799286\n",
      "epoch 43 i 10098 loss 1.292408989071846\n",
      "epoch 43 i 11098 loss 1.29315913438797\n",
      "epoch 43 i 12098 loss 1.2931352669000626\n",
      "epoch 43 loss 1.2732471227645874: e nationalism the culturbus the new world roots cushical overlop the first union taxes new york four\n",
      "epoch 44 i 891 loss 1.2898979549407958\n",
      "epoch 44 i 1891 loss 1.2903661538362503\n",
      "epoch 44 i 2891 loss 1.2911939920186997\n",
      "epoch 44 i 3891 loss 1.2925114287137984\n",
      "epoch 44 i 4891 loss 1.2918857407569886\n",
      "epoch 44 i 5891 loss 1.2901756798028945\n",
      "epoch 44 i 6891 loss 1.2922453587055207\n",
      "epoch 44 i 7891 loss 1.2902836793661117\n",
      "epoch 44 i 8891 loss 1.292358865380287\n",
      "epoch 44 i 9891 loss 1.2906436828374863\n",
      "epoch 44 i 10891 loss 1.2897661958932876\n",
      "epoch 44 i 11891 loss 1.2892760241031647\n",
      "epoch 44 loss 1.2739797830581665: ree six were refer commonly biays one two include connotain bradual works heaved operations and sugg\n",
      "epoch 45 i 684 loss 1.288737855553627\n",
      "epoch 45 i 1684 loss 1.2893199363946914\n",
      "epoch 45 i 2684 loss 1.2898329625129699\n",
      "epoch 45 i 3684 loss 1.2892989885807038\n",
      "epoch 45 i 4684 loss 1.287518583059311\n",
      "epoch 45 i 5684 loss 1.2890702505111695\n",
      "epoch 45 i 6684 loss 1.288191688656807\n",
      "epoch 45 i 7684 loss 1.2872647842168807\n",
      "epoch 45 i 8684 loss 1.2868883316516877\n",
      "epoch 45 i 9684 loss 1.2879029837846756\n",
      "epoch 45 i 10684 loss 1.2885620154142379\n",
      "epoch 45 i 11684 loss 1.2865697627067565\n",
      "epoch 45 loss 1.2715860605239868: e classical station are involved at conclhiring that features have negatased the acts an automatedly\n",
      "epoch 46 i 477 loss 1.2863936446905135\n",
      "epoch 46 i 1477 loss 1.2844577791690825\n",
      "epoch 46 i 2477 loss 1.2853894500732421\n",
      "epoch 46 i 3477 loss 1.285078962445259\n",
      "epoch 46 i 4477 loss 1.2877281049489975\n",
      "epoch 46 i 5477 loss 1.2858521972894668\n",
      "epoch 46 i 6477 loss 1.2842664927244187\n",
      "epoch 46 i 7477 loss 1.2835631368160247\n",
      "epoch 46 i 8477 loss 1.2842097135782242\n",
      "epoch 46 i 9477 loss 1.283971834421158\n",
      "epoch 46 i 10477 loss 1.2837334080934524\n",
      "epoch 46 i 11477 loss 1.2843046643733977\n",
      "epoch 46 loss 1.3229366540908813: e legacy memore for give physicials crediments a bounding the memory and the united states as does b\n",
      "epoch 47 i 270 loss 1.2840136831998825\n",
      "epoch 47 i 1270 loss 1.284389692902565\n",
      "epoch 47 i 2270 loss 1.2818265379667282\n",
      "epoch 47 i 3270 loss 1.2824627470970154\n",
      "epoch 47 i 4270 loss 1.283024290204048\n",
      "epoch 47 i 5270 loss 1.282108055472374\n",
      "epoch 47 i 6270 loss 1.2810403567552566\n",
      "epoch 47 i 7270 loss 1.2819686975479125\n",
      "epoch 47 i 8270 loss 1.2829281958341598\n",
      "epoch 47 i 9270 loss 1.2826595944166184\n",
      "epoch 47 i 10270 loss 1.2786743975877761\n",
      "epoch 47 i 11270 loss 1.282390130996704\n",
      "epoch 47 loss 1.307447910308838: e wifce usually all awkiped the troll un one eight th curcle needs agay has bit the first manimal wh\n",
      "epoch 48 i 63 loss 1.2811949191093446\n",
      "epoch 48 i 1063 loss 1.2785452927350998\n",
      "epoch 48 i 2063 loss 1.282290630698204\n",
      "epoch 48 i 3063 loss 1.2794942073822022\n",
      "epoch 48 i 4063 loss 1.278618907570839\n",
      "epoch 48 i 5063 loss 1.2778756840229035\n",
      "epoch 48 i 6063 loss 1.2792911233901978\n",
      "epoch 48 i 7063 loss 1.2794417853355409\n",
      "epoch 48 i 8063 loss 1.2787439917325973\n",
      "epoch 48 i 9063 loss 1.2775024443864822\n",
      "epoch 48 i 10063 loss 1.2783536165952682\n",
      "epoch 48 i 11063 loss 1.2776683230400085\n",
      "epoch 48 i 12063 loss 1.2770634226799011\n",
      "epoch 48 loss 1.2468512058258057: at thus the revolution of the earliest some felling sound of some actually are there described the e\n",
      "epoch 49 i 856 loss 1.278721471428871\n",
      "epoch 49 i 1856 loss 1.2763303756713866\n",
      "epoch 49 i 2856 loss 1.278307911992073\n",
      "epoch 49 i 3856 loss 1.2757689852714538\n",
      "epoch 49 i 4856 loss 1.2757473565340043\n",
      "epoch 49 i 5856 loss 1.276282504439354\n",
      "epoch 49 i 6856 loss 1.2751640231609345\n",
      "epoch 49 i 7856 loss 1.2769398032426833\n",
      "epoch 49 i 8856 loss 1.2782326282262801\n",
      "epoch 49 i 9856 loss 1.2765451817512512\n",
      "epoch 49 i 10856 loss 1.2744786405563355\n",
      "epoch 49 i 11856 loss 1.2740656535625459\n",
      "epoch 49 loss 1.2145133018493652: e united standing autonification now its drive common assembly mexicions establishmeck deated but in\n",
      "epoch 50 i 649 loss 1.275866436600685\n",
      "epoch 50 i 1649 loss 1.2740617861747743\n",
      "epoch 50 i 2649 loss 1.2754108958244323\n",
      "epoch 50 i 3649 loss 1.2738019223213195\n",
      "epoch 50 i 4649 loss 1.2741027981042863\n",
      "epoch 50 i 5649 loss 1.27354674077034\n",
      "epoch 50 i 6649 loss 1.2735576862096787\n",
      "epoch 50 i 7649 loss 1.272932515025139\n",
      "epoch 50 i 8649 loss 1.272772162914276\n",
      "epoch 50 i 9649 loss 1.2730002183914184\n",
      "epoch 50 i 10649 loss 1.2727333362102509\n",
      "epoch 50 i 11649 loss 1.2714420092105865\n",
      "epoch 50 loss 1.2805156707763672: e speeds the beswer described the action and ko career belated by a work of the signal part of all t\n",
      "epoch 51 i 442 loss 1.272591824889183\n",
      "epoch 51 i 1442 loss 1.2738360105752944\n",
      "epoch 51 i 2442 loss 1.2730385026931763\n",
      "epoch 51 i 3442 loss 1.2728762897253036\n",
      "epoch 51 i 4442 loss 1.2739420694112777\n",
      "epoch 51 i 5442 loss 1.2728807092905046\n",
      "epoch 51 i 6442 loss 1.2703237121105193\n",
      "epoch 51 i 7442 loss 1.2700674066543578\n",
      "epoch 51 i 8442 loss 1.2708183571100236\n",
      "epoch 51 i 9442 loss 1.2713891956806183\n",
      "epoch 51 i 10442 loss 1.2682339335680008\n",
      "epoch 51 i 11442 loss 1.2715464339256286\n",
      "epoch 51 loss 1.232400894165039: e use of that twice can health the players and cantille that or compella global can help two zero ze\n",
      "epoch 52 i 235 loss 1.2692736512422562\n",
      "epoch 52 i 1235 loss 1.2691727039813996\n",
      "epoch 52 i 2235 loss 1.2706388936042785\n",
      "epoch 52 i 3235 loss 1.2688539416790008\n",
      "epoch 52 i 4235 loss 1.2687589586973191\n",
      "epoch 52 i 5235 loss 1.2682855609655381\n",
      "epoch 52 i 6235 loss 1.266843068242073\n",
      "epoch 52 i 7235 loss 1.267769514322281\n",
      "epoch 52 i 8235 loss 1.2698104734420776\n",
      "epoch 52 i 9235 loss 1.2698602648973465\n",
      "epoch 52 i 10235 loss 1.2709471877813339\n",
      "epoch 52 i 11235 loss 1.268800968527794\n",
      "epoch 52 loss 1.2362713813781738: e persamism has a serial personnel as a more text power for persistation the first republic regimes \n",
      "epoch 53 i 28 loss 1.2672101541757583\n",
      "epoch 53 i 1028 loss 1.2692808301448821\n",
      "epoch 53 i 2028 loss 1.2659279857873917\n",
      "epoch 53 i 3028 loss 1.268058547616005\n",
      "epoch 53 i 4028 loss 1.267586552143097\n",
      "epoch 53 i 5028 loss 1.2665603343248368\n",
      "epoch 53 i 6028 loss 1.2679946247339249\n",
      "epoch 53 i 7028 loss 1.2690438104867936\n",
      "epoch 53 i 8028 loss 1.2643642117977143\n",
      "epoch 53 i 9028 loss 1.2655848429203034\n",
      "epoch 53 i 10028 loss 1.2654397593736648\n",
      "epoch 53 i 11028 loss 1.2656776071786882\n",
      "epoch 53 i 12028 loss 1.2653073828220367\n",
      "epoch 53 loss 1.2665492296218872: e restrict of the most form all white had all brade for work which constantly more genometric plain \n",
      "epoch 54 i 821 loss 1.2646098037958144\n",
      "epoch 54 i 1821 loss 1.2659452562332154\n",
      "epoch 54 i 2821 loss 1.2657457681894302\n",
      "epoch 54 i 3821 loss 1.2656395262479783\n",
      "epoch 54 i 4821 loss 1.264331148147583\n",
      "epoch 54 i 5821 loss 1.264851333141327\n",
      "epoch 54 i 6821 loss 1.2667402334213256\n",
      "epoch 54 i 7821 loss 1.2636079422235489\n",
      "epoch 54 i 8821 loss 1.2653301615715027\n",
      "epoch 54 i 9821 loss 1.2649382874965667\n",
      "epoch 54 i 10821 loss 1.2627618291378022\n",
      "epoch 54 i 11821 loss 1.2629644595384597\n",
      "epoch 54 loss 1.288283348083496: ere in europe between when featured for one nine two zero dani including seake bara was spreaker the\n",
      "epoch 55 i 614 loss 1.2637356884479523\n",
      "epoch 55 i 1614 loss 1.2640042258501052\n",
      "epoch 55 i 2614 loss 1.2622280284166336\n",
      "epoch 55 i 3614 loss 1.2632585513591765\n",
      "epoch 55 i 4614 loss 1.2645170586109162\n",
      "epoch 55 i 5614 loss 1.264995283126831\n",
      "epoch 55 i 6614 loss 1.2629006534814835\n",
      "epoch 55 i 7614 loss 1.2629126436710358\n",
      "epoch 55 i 8614 loss 1.2632814987897873\n",
      "epoch 55 i 9614 loss 1.2630821868181228\n",
      "epoch 55 i 10614 loss 1.2623486731052398\n",
      "epoch 55 i 11614 loss 1.2601683948040008\n",
      "epoch 55 loss 1.2810918092727661: is sent judges of career scahol related to who marcquid power by revellent frag digic for prespected\n",
      "epoch 56 i 407 loss 1.2616429563760758\n",
      "epoch 56 i 1407 loss 1.2612893092632294\n",
      "epoch 56 i 2407 loss 1.2616520994901657\n",
      "epoch 56 i 3407 loss 1.259575599193573\n",
      "epoch 56 i 4407 loss 1.2625032684803008\n",
      "epoch 56 i 5407 loss 1.2613576517105103\n",
      "epoch 56 i 6407 loss 1.25986272418499\n",
      "epoch 56 i 7407 loss 1.2605510277748109\n",
      "epoch 56 i 8407 loss 1.2586886060237885\n",
      "epoch 56 i 9407 loss 1.2596494220495225\n",
      "epoch 56 i 10407 loss 1.2596351815462112\n",
      "epoch 56 i 11407 loss 1.2608743634223938\n",
      "epoch 56 loss 1.3004717826843262: e real appearance is merifluence then every he was a set could to show a prove in one nine eight thr\n",
      "epoch 57 i 200 loss 1.2598864132165908\n",
      "epoch 57 i 1200 loss 1.2597130706310273\n",
      "epoch 57 i 2200 loss 1.2577330179214476\n",
      "epoch 57 i 3200 loss 1.2591845455169677\n",
      "epoch 57 i 4200 loss 1.262345695734024\n",
      "epoch 57 i 5200 loss 1.259460344314575\n",
      "epoch 57 i 6200 loss 1.2593517775535583\n",
      "epoch 57 i 7200 loss 1.2577912681102752\n",
      "epoch 57 i 8200 loss 1.2587719532251358\n",
      "epoch 57 i 9200 loss 1.2587009307146073\n",
      "epoch 57 i 10200 loss 1.258728698849678\n",
      "epoch 57 i 11200 loss 1.2578997774124145\n",
      "epoch 57 i 12200 loss 1.2572902761697768\n",
      "epoch 57 loss 1.2042591571807861: e republic defeats the term of include a defeated by an english form is used the continuous the word\n",
      "epoch 58 i 993 loss 1.2574641231298447\n",
      "epoch 58 i 1993 loss 1.2567922090291976\n",
      "epoch 58 i 2993 loss 1.256880326986313\n",
      "epoch 58 i 3993 loss 1.2570720723867417\n",
      "epoch 58 i 4993 loss 1.2588771877288818\n",
      "epoch 58 i 5993 loss 1.2590647419691086\n",
      "epoch 58 i 6993 loss 1.2567178922891618\n",
      "epoch 58 i 7993 loss 1.256126319050789\n",
      "epoch 58 i 8993 loss 1.2564497681856155\n",
      "epoch 58 i 9993 loss 1.255499875664711\n",
      "epoch 58 i 10993 loss 1.256592460513115\n",
      "epoch 58 i 11993 loss 1.2560471559762956\n",
      "epoch 58 loss 1.2423782348632812: is establishments points at the early needs in the early egg proclamattical tibe the denominate of t\n",
      "epoch 59 i 786 loss 1.2554027870893478\n",
      "epoch 59 i 1786 loss 1.2544222285747528\n",
      "epoch 59 i 2786 loss 1.25583961892128\n",
      "epoch 59 i 3786 loss 1.25538463139534\n",
      "epoch 59 i 4786 loss 1.2529423464536666\n",
      "epoch 59 i 5786 loss 1.2545593501329422\n",
      "epoch 59 i 6786 loss 1.2539776725769043\n",
      "epoch 59 i 7786 loss 1.2546529797315598\n",
      "epoch 59 i 8786 loss 1.2564124755859376\n",
      "epoch 59 i 9786 loss 1.253664380311966\n",
      "epoch 59 i 10786 loss 1.2535968933105468\n",
      "epoch 59 i 11786 loss 1.256491661787033\n",
      "epoch 59 loss 1.2393590211868286: e early while use a century similar theory and is annound somether first around the kowal removing s\n",
      "epoch 60 i 579 loss 1.255303557395935\n",
      "epoch 60 i 1579 loss 1.252808318734169\n",
      "epoch 60 i 2579 loss 1.2550268028974534\n",
      "epoch 60 i 3579 loss 1.2538745056390763\n",
      "epoch 60 i 4579 loss 1.2531633268594742\n",
      "epoch 60 i 5579 loss 1.2530820931196214\n",
      "epoch 60 i 6579 loss 1.2532969096899031\n",
      "epoch 60 i 7579 loss 1.252708113670349\n",
      "epoch 60 i 8579 loss 1.2518233697414398\n",
      "epoch 60 i 9579 loss 1.2528598806858062\n",
      "epoch 60 i 10579 loss 1.2523244715929032\n",
      "epoch 60 i 11579 loss 1.252657609820366\n",
      "epoch 60 loss 1.3012338876724243: e personity confirms to digital musicians the instures of also paylot would etc and red to present b\n",
      "epoch 61 i 372 loss 1.252344158768654\n",
      "epoch 61 i 1372 loss 1.251455096244812\n",
      "epoch 61 i 2372 loss 1.2496224638223647\n",
      "epoch 61 i 3372 loss 1.2532782930135726\n",
      "epoch 61 i 4372 loss 1.251540677547455\n",
      "epoch 61 i 5372 loss 1.253971951842308\n",
      "epoch 61 i 6372 loss 1.251262504339218\n",
      "epoch 61 i 7372 loss 1.2510902686119079\n",
      "epoch 61 i 8372 loss 1.2498545695543288\n",
      "epoch 61 i 9372 loss 1.2512032265663147\n",
      "epoch 61 i 10372 loss 1.248932434439659\n",
      "epoch 61 i 11372 loss 1.2504894268512725\n",
      "epoch 61 loss 1.22635817527771: e son is commonwealth of burier xuries steep s whign often placed when an or involved than the end b\n",
      "epoch 62 i 165 loss 1.249727124094963\n",
      "epoch 62 i 1165 loss 1.2501919487714768\n",
      "epoch 62 i 2165 loss 1.2505538622140884\n",
      "epoch 62 i 3165 loss 1.250534828901291\n",
      "epoch 62 i 4165 loss 1.249409434914589\n",
      "epoch 62 i 5165 loss 1.251633192062378\n",
      "epoch 62 i 6165 loss 1.2494641995429994\n",
      "epoch 62 i 7165 loss 1.24845863032341\n",
      "epoch 62 i 8165 loss 1.2474048923254013\n",
      "epoch 62 i 9165 loss 1.2486894830465316\n",
      "epoch 62 i 10165 loss 1.2492058680057525\n",
      "epoch 62 i 11165 loss 1.250573707342148\n",
      "epoch 62 i 12165 loss 1.2483574154376984\n",
      "epoch 62 loss 1.2233037948608398: e populars radically success in the blass as for jumping april it is the standard traditional penetr\n",
      "epoch 63 i 958 loss 1.248292332291603\n",
      "epoch 63 i 1958 loss 1.2483140412569047\n",
      "epoch 63 i 2958 loss 1.2465565586090088\n",
      "epoch 63 i 3958 loss 1.2499496446847915\n",
      "epoch 63 i 4958 loss 1.2482292487621307\n",
      "epoch 63 i 5958 loss 1.24815540766716\n",
      "epoch 63 i 6958 loss 1.2452841583490373\n",
      "epoch 63 i 7958 loss 1.2476660650968552\n",
      "epoch 63 i 8958 loss 1.2478536794185637\n",
      "epoch 63 i 9958 loss 1.24687697327137\n",
      "epoch 63 i 10958 loss 1.2474772284030915\n",
      "epoch 63 i 11958 loss 1.2455495203733444\n",
      "epoch 63 loss 1.298008680343628: e one of several now allow michter to regulation are often at although the radical west years one th\n",
      "epoch 64 i 751 loss 1.2468533664941788\n",
      "epoch 64 i 1751 loss 1.245591621518135\n",
      "epoch 64 i 2751 loss 1.2453697317838668\n",
      "epoch 64 i 3751 loss 1.2480864528417588\n",
      "epoch 64 i 4751 loss 1.2465300053358077\n",
      "epoch 64 i 5751 loss 1.2452056818008423\n",
      "epoch 64 i 6751 loss 1.245455779671669\n",
      "epoch 64 i 7751 loss 1.2483469396829605\n",
      "epoch 64 i 8751 loss 1.2474756860733032\n",
      "epoch 64 i 9751 loss 1.2460700153112412\n",
      "epoch 64 i 10751 loss 1.246485586166382\n",
      "epoch 64 i 11751 loss 1.245979688167572\n",
      "epoch 64 loss 1.2908176183700562: e society of laster transfers is dominated in colonies radio a bishop for field the united at instan\n",
      "epoch 65 i 544 loss 1.2448798304796218\n",
      "epoch 65 i 1544 loss 1.2424502700567246\n",
      "epoch 65 i 2544 loss 1.2450363820791244\n",
      "epoch 65 i 3544 loss 1.246091183900833\n",
      "epoch 65 i 4544 loss 1.246505557179451\n",
      "epoch 65 i 5544 loss 1.245036087989807\n",
      "epoch 65 i 6544 loss 1.2458821699619294\n",
      "epoch 65 i 7544 loss 1.2449790065288544\n",
      "epoch 65 i 8544 loss 1.2444628828763962\n",
      "epoch 65 i 9544 loss 1.2452731521129607\n",
      "epoch 65 i 10544 loss 1.243633318901062\n",
      "epoch 65 i 11544 loss 1.2462355508804321\n",
      "epoch 65 loss 1.1776061058044434: eme outside behinds in the institution is under to demonting with long family rochester to long unde\n",
      "epoch 66 i 337 loss 1.2434194536209107\n",
      "epoch 66 i 1337 loss 1.244003737807274\n",
      "epoch 66 i 2337 loss 1.2433349131345748\n",
      "epoch 66 i 3337 loss 1.242964695572853\n",
      "epoch 66 i 4337 loss 1.2443167423009873\n",
      "epoch 66 i 5337 loss 1.2437412202358247\n",
      "epoch 66 i 6337 loss 1.243738332748413\n",
      "epoch 66 i 7337 loss 1.2444226109981538\n",
      "epoch 66 i 8337 loss 1.2418608552217483\n",
      "epoch 66 i 9337 loss 1.2427513055801391\n",
      "epoch 66 i 10337 loss 1.2430946497917175\n",
      "epoch 66 i 11337 loss 1.2431904462575913\n",
      "epoch 66 loss 1.2692850828170776: e union governors of the bermedian as many the city hard wwo schedualty it is still the apolonial us\n",
      "epoch 67 i 130 loss 1.245484244942665\n",
      "epoch 67 i 1130 loss 1.2426155660152436\n",
      "epoch 67 i 2130 loss 1.2402691308259963\n",
      "epoch 67 i 3130 loss 1.2423647047281265\n",
      "epoch 67 i 4130 loss 1.2407647520303726\n",
      "epoch 67 i 5130 loss 1.2412340111732483\n",
      "epoch 67 i 6130 loss 1.2405265908241272\n",
      "epoch 67 i 7130 loss 1.2414860512018204\n",
      "epoch 67 i 8130 loss 1.241228929400444\n",
      "epoch 67 i 9130 loss 1.2416841980218887\n",
      "epoch 67 i 10130 loss 1.242291046500206\n",
      "epoch 67 i 11130 loss 1.2422258125543595\n",
      "epoch 67 i 12130 loss 1.2428704994916915\n",
      "epoch 67 loss 1.2089258432388306: e normal with mountains all attemptin is adjectively revolution of allows that of effect engines any\n",
      "epoch 68 i 923 loss 1.2421563874483108\n",
      "epoch 68 i 1923 loss 1.2395440039634704\n",
      "epoch 68 i 2923 loss 1.2418941019773484\n",
      "epoch 68 i 3923 loss 1.2405966089963913\n",
      "epoch 68 i 4923 loss 1.240918420433998\n",
      "epoch 68 i 5923 loss 1.2428319137096404\n",
      "epoch 68 i 6923 loss 1.238552868962288\n",
      "epoch 68 i 7923 loss 1.2428784409761429\n",
      "epoch 68 i 8923 loss 1.2398039869070052\n",
      "epoch 68 i 9923 loss 1.2407874335050584\n",
      "epoch 68 i 10923 loss 1.241265142083168\n",
      "epoch 68 i 11923 loss 1.2425823539495469\n",
      "epoch 68 loss 1.2120720148086548: e term s doctrinecton dige by their dedicated boarding the other of the uters balf that the density \n",
      "epoch 69 i 716 loss 1.239658012509346\n",
      "epoch 69 i 1716 loss 1.2401245439052582\n",
      "epoch 69 i 2716 loss 1.238648643732071\n",
      "epoch 69 i 3716 loss 1.238439833164215\n",
      "epoch 69 i 4716 loss 1.2387029501199722\n",
      "epoch 69 i 5716 loss 1.2382640533447267\n",
      "epoch 69 i 6716 loss 1.2390794271230698\n",
      "epoch 69 i 7716 loss 1.2406920347213746\n",
      "epoch 69 i 8716 loss 1.238257467150688\n",
      "epoch 69 i 9716 loss 1.2376388635635376\n",
      "epoch 69 i 10716 loss 1.2365502388477325\n",
      "epoch 69 i 11716 loss 1.240400740623474\n",
      "epoch 69 loss 1.2037718296051025: e digs then general nonle digital come to four one digitally assignment thy creates some often discr\n",
      "epoch 70 i 509 loss 1.23838563144207\n",
      "epoch 70 i 1509 loss 1.2390933413505554\n",
      "epoch 70 i 2509 loss 1.2390538985729218\n",
      "epoch 70 i 3509 loss 1.239261099934578\n",
      "epoch 70 i 4509 loss 1.2373556030988693\n",
      "epoch 70 i 5509 loss 1.2381775251626967\n",
      "epoch 70 i 6509 loss 1.2373126497268676\n",
      "epoch 70 i 7509 loss 1.2384152555465697\n",
      "epoch 70 i 8509 loss 1.2360737173557281\n",
      "epoch 70 i 9509 loss 1.2384075974225999\n",
      "epoch 70 i 10509 loss 1.2377599060535431\n",
      "epoch 70 i 11509 loss 1.2376665961742401\n",
      "epoch 70 loss 1.3015875816345215: e ages of his sony markford that this digplated arithmiculture magice him at twenty or shall that el\n",
      "epoch 71 i 302 loss 1.237416880130768\n",
      "epoch 71 i 1302 loss 1.2354937553405763\n",
      "epoch 71 i 2302 loss 1.2362910445928574\n",
      "epoch 71 i 3302 loss 1.2380662387609482\n",
      "epoch 71 i 4302 loss 1.236359087705612\n",
      "epoch 71 i 5302 loss 1.2363924514055251\n",
      "epoch 71 i 6302 loss 1.2351061755418777\n",
      "epoch 71 i 7302 loss 1.2370065406560897\n",
      "epoch 71 i 8302 loss 1.2350777691602708\n",
      "epoch 71 i 9302 loss 1.235806841135025\n",
      "epoch 71 i 10302 loss 1.2349317500591277\n",
      "epoch 71 i 11302 loss 1.2375982847213745\n",
      "epoch 71 loss 1.2456915378570557: at however when the north maximizi was a pcf is released that were closed what led general antimus v\n",
      "epoch 72 i 95 loss 1.2368338248729707\n",
      "epoch 72 i 1095 loss 1.2360242019891738\n",
      "epoch 72 i 2095 loss 1.234747683763504\n",
      "epoch 72 i 3095 loss 1.2348850611448288\n",
      "epoch 72 i 4095 loss 1.235527102828026\n",
      "epoch 72 i 5095 loss 1.236098865389824\n",
      "epoch 72 i 6095 loss 1.234726762533188\n",
      "epoch 72 i 7095 loss 1.2352453067302704\n",
      "epoch 72 i 8095 loss 1.232256997346878\n",
      "epoch 72 i 9095 loss 1.2355756876468658\n",
      "epoch 72 i 10095 loss 1.234798037648201\n",
      "epoch 72 i 11095 loss 1.2347887034416198\n",
      "epoch 72 i 12095 loss 1.2354104181528092\n",
      "epoch 72 loss 1.2522788047790527: e innovation or many artillery many described there is member of the fluctuation foot may drive whic\n",
      "epoch 73 i 888 loss 1.2367684371471406\n",
      "epoch 73 i 1888 loss 1.2335067987442017\n",
      "epoch 73 i 2888 loss 1.2341465955972672\n",
      "epoch 73 i 3888 loss 1.2327055490016938\n",
      "epoch 73 i 4888 loss 1.2320962142944336\n",
      "epoch 73 i 5888 loss 1.2353126500844955\n",
      "epoch 73 i 6888 loss 1.2334476675987243\n",
      "epoch 73 i 7888 loss 1.2345368634462357\n",
      "epoch 73 i 8888 loss 1.2314538141489029\n",
      "epoch 73 i 9888 loss 1.2331638748645783\n",
      "epoch 73 i 10888 loss 1.2348197714090348\n",
      "epoch 73 i 11888 loss 1.2332071076631546\n",
      "epoch 73 loss 1.1718111038208008: e user there es maku alcoration information sandation french deities and the dark of a shyttle of de\n",
      "epoch 74 i 681 loss 1.2322659413814545\n",
      "epoch 74 i 1681 loss 1.2334882459640504\n",
      "epoch 74 i 2681 loss 1.2344167807102204\n",
      "epoch 74 i 3681 loss 1.2322666499614716\n",
      "epoch 74 i 4681 loss 1.2323956737518311\n",
      "epoch 74 i 5681 loss 1.2338258080482483\n",
      "epoch 74 i 6681 loss 1.2309297090768814\n",
      "epoch 74 i 7681 loss 1.2312539613246918\n",
      "epoch 74 i 8681 loss 1.2341978405714036\n",
      "epoch 74 i 9681 loss 1.231870136141777\n",
      "epoch 74 i 10681 loss 1.2328838945627212\n",
      "epoch 74 i 11681 loss 1.2301801692247392\n",
      "epoch 74 loss 1.2228734493255615: e tremble of typezg was classically supported at the end of one three four th and further an indoor \n",
      "epoch 75 i 474 loss 1.2334549877643586\n",
      "epoch 75 i 1474 loss 1.229604063153267\n",
      "epoch 75 i 2474 loss 1.2321741050481796\n",
      "epoch 75 i 3474 loss 1.2311531938314437\n",
      "epoch 75 i 4474 loss 1.231913741350174\n",
      "epoch 75 i 5474 loss 1.2316520540714264\n",
      "epoch 75 i 6474 loss 1.231595903277397\n",
      "epoch 75 i 7474 loss 1.2322998396158218\n",
      "epoch 75 i 8474 loss 1.2316185024976731\n",
      "epoch 75 i 9474 loss 1.2317997932434082\n",
      "epoch 75 i 10474 loss 1.2302757198810577\n",
      "epoch 75 i 11474 loss 1.2301332134008407\n",
      "epoch 75 loss 1.2037403583526611: e anniversation to the united properties in the formerly describes famous is one five nine one anoth\n",
      "epoch 76 i 267 loss 1.2310034135580064\n",
      "epoch 76 i 1267 loss 1.230375679731369\n",
      "epoch 76 i 2267 loss 1.2315605627298356\n",
      "epoch 76 i 3267 loss 1.228868310213089\n",
      "epoch 76 i 4267 loss 1.2323526600599288\n",
      "epoch 76 i 5267 loss 1.2309556679725646\n",
      "epoch 76 i 6267 loss 1.228282094836235\n",
      "epoch 76 i 7267 loss 1.2312880228757859\n",
      "epoch 76 i 8267 loss 1.2279605717658997\n",
      "epoch 76 i 9267 loss 1.2318660414218903\n",
      "epoch 76 i 10267 loss 1.2298462915420532\n",
      "epoch 76 i 11267 loss 1.2293721734285354\n",
      "epoch 76 loss 1.2386177778244019: e follow of jones there were government she as brown that a greats has to see victoria in the tresse\n",
      "epoch 77 i 60 loss 1.2281489157676697\n",
      "epoch 77 i 1060 loss 1.2294853605031968\n",
      "epoch 77 i 2060 loss 1.2314767795801163\n",
      "epoch 77 i 3060 loss 1.2272379139661789\n",
      "epoch 77 i 4060 loss 1.2292631912231444\n",
      "epoch 77 i 5060 loss 1.2294959305524826\n",
      "epoch 77 i 6060 loss 1.2273307211399078\n",
      "epoch 77 i 7060 loss 1.2293749229907989\n",
      "epoch 77 i 8060 loss 1.2300717971324922\n",
      "epoch 77 i 9060 loss 1.229718523144722\n",
      "epoch 77 i 10060 loss 1.229493490934372\n",
      "epoch 77 i 11060 loss 1.229402515053749\n",
      "epoch 77 i 12060 loss 1.2295236859321594\n",
      "epoch 77 loss 1.2097378969192505: e direction of people was a six zero metal one for them the new monotative small purposes the draw w\n",
      "epoch 78 i 853 loss 1.2279072773456574\n",
      "epoch 78 i 1853 loss 1.2292846174240113\n",
      "epoch 78 i 2853 loss 1.2286020523309709\n",
      "epoch 78 i 3853 loss 1.2279028893709183\n",
      "epoch 78 i 4853 loss 1.2268687014579773\n",
      "epoch 78 i 5853 loss 1.2279452048540116\n",
      "epoch 78 i 6853 loss 1.227366167664528\n",
      "epoch 78 i 7853 loss 1.2272377758026123\n",
      "epoch 78 i 8853 loss 1.2276020027399064\n",
      "epoch 78 i 9853 loss 1.2289730874300002\n",
      "epoch 78 i 10853 loss 1.229173159122467\n",
      "epoch 78 i 11853 loss 1.2282563387155532\n",
      "epoch 78 loss 1.230961799621582: e state state rumnie system and for black one nine six poorism mp shepards are less revealed in have\n",
      "epoch 79 i 646 loss 1.2270265216827392\n",
      "epoch 79 i 1646 loss 1.22632925760746\n",
      "epoch 79 i 2646 loss 1.2290625095367431\n",
      "epoch 79 i 3646 loss 1.2261423618793488\n",
      "epoch 79 i 4646 loss 1.2278208942413331\n",
      "epoch 79 i 5646 loss 1.226495832324028\n",
      "epoch 79 i 6646 loss 1.2232454243898392\n",
      "epoch 79 i 7646 loss 1.2278582706451415\n",
      "epoch 79 i 8646 loss 1.22471300137043\n",
      "epoch 79 i 9646 loss 1.2255293920040131\n",
      "epoch 79 i 10646 loss 1.2269253669977187\n",
      "epoch 79 i 11646 loss 1.224686237215996\n",
      "epoch 79 loss 1.2493577003479004: e council formal famous meanwhere there are revooking argued devoted to dechler the final tremender \n",
      "epoch 80 i 439 loss 1.2272567160129546\n",
      "epoch 80 i 1439 loss 1.2265958133935928\n",
      "epoch 80 i 2439 loss 1.2249261864423753\n",
      "epoch 80 i 3439 loss 1.2265880938768388\n",
      "epoch 80 i 4439 loss 1.2251665447950364\n",
      "epoch 80 i 5439 loss 1.2246379543542862\n",
      "epoch 80 i 6439 loss 1.2253665739297868\n",
      "epoch 80 i 7439 loss 1.225316628932953\n",
      "epoch 80 i 8439 loss 1.2246000611782073\n",
      "epoch 80 i 9439 loss 1.2237461661100388\n",
      "epoch 80 i 10439 loss 1.2252117469310762\n",
      "epoch 80 i 11439 loss 1.2257404352426529\n",
      "epoch 80 loss 1.2409625053405762: e time of ak largel the term coasts of how in england time bond land barought sticketing section as \n",
      "epoch 81 i 232 loss 1.2256287417411804\n",
      "epoch 81 i 1232 loss 1.2258332166671753\n",
      "epoch 81 i 2232 loss 1.2236603293418884\n",
      "epoch 81 i 3232 loss 1.2260171695947648\n",
      "epoch 81 i 4232 loss 1.225508875131607\n",
      "epoch 81 i 5232 loss 1.224142043352127\n",
      "epoch 81 i 6232 loss 1.2250682948827742\n",
      "epoch 81 i 7232 loss 1.224323288321495\n",
      "epoch 81 i 8232 loss 1.2233402037620544\n",
      "epoch 81 i 9232 loss 1.2261758316755296\n",
      "epoch 81 i 10232 loss 1.2228585431575776\n",
      "epoch 81 i 11232 loss 1.223812307715416\n",
      "epoch 81 loss 1.21773362159729: e instrument the that dorothy of governheides that itself the motion of government its commands with\n",
      "epoch 82 i 25 loss 1.2251230068206787\n",
      "epoch 82 i 1025 loss 1.2261041758060456\n",
      "epoch 82 i 2025 loss 1.225147135257721\n",
      "epoch 82 i 3025 loss 1.2217944850921632\n",
      "epoch 82 i 4025 loss 1.2236600977182388\n",
      "epoch 82 i 5025 loss 1.2232625550031662\n",
      "epoch 82 i 6025 loss 1.2235996963977813\n",
      "epoch 82 i 7025 loss 1.222612748503685\n",
      "epoch 82 i 8025 loss 1.2242602286338806\n",
      "epoch 82 i 9025 loss 1.2214896862506865\n",
      "epoch 82 i 10025 loss 1.2229653625488281\n",
      "epoch 82 i 11025 loss 1.2225267856121063\n",
      "epoch 82 i 12025 loss 1.2221866943836213\n",
      "epoch 82 loss 1.228566288948059: e ground of thoughts and number the same detroit designed a redeant as rotation has interest make th\n",
      "epoch 83 i 818 loss 1.2227754504680635\n",
      "epoch 83 i 1818 loss 1.2232830142974853\n",
      "epoch 83 i 2818 loss 1.2229453078508377\n",
      "epoch 83 i 3818 loss 1.2226928758621216\n",
      "epoch 83 i 4818 loss 1.2236245497465135\n",
      "epoch 83 i 5818 loss 1.2208006938695908\n",
      "epoch 83 i 6818 loss 1.2215138710737228\n",
      "epoch 83 i 7818 loss 1.2225260121822357\n",
      "epoch 83 i 8818 loss 1.2225579649209977\n",
      "epoch 83 i 9818 loss 1.2225358922481537\n",
      "epoch 83 i 10818 loss 1.2197679872512817\n",
      "epoch 83 i 11818 loss 1.2211665961742402\n",
      "epoch 83 loss 1.2142928838729858: e mirackmen of hubbrew in the fourth as of a space of the sanse bee difficult to the two of a six fr\n",
      "epoch 84 i 611 loss 1.2214565603733063\n",
      "epoch 84 i 1611 loss 1.2211184628009797\n",
      "epoch 84 i 2611 loss 1.2220243895053864\n",
      "epoch 84 i 3611 loss 1.2221293407678604\n",
      "epoch 84 i 4611 loss 1.2215685391426085\n",
      "epoch 84 i 5611 loss 1.2225698934793472\n",
      "epoch 84 i 6611 loss 1.2209373100996017\n",
      "epoch 84 i 7611 loss 1.2204797184467315\n",
      "epoch 84 i 8611 loss 1.2216080647706986\n",
      "epoch 84 i 9611 loss 1.2201819229125976\n",
      "epoch 84 i 10611 loss 1.2223655709028245\n",
      "epoch 84 i 11611 loss 1.219645800113678\n",
      "epoch 84 loss 1.246286392211914: e united small from radicals fool held one of a new defence and such a secholuttarily the asps on re\n",
      "epoch 85 i 404 loss 1.2204626041650772\n",
      "epoch 85 i 1404 loss 1.221189065337181\n",
      "epoch 85 i 2404 loss 1.221982150554657\n",
      "epoch 85 i 3404 loss 1.2196349816322327\n",
      "epoch 85 i 4404 loss 1.2197739325761796\n",
      "epoch 85 i 5404 loss 1.2210412279367446\n",
      "epoch 85 i 6404 loss 1.2199603320360184\n",
      "epoch 85 i 7404 loss 1.2198379180431367\n",
      "epoch 85 i 8404 loss 1.218873176574707\n",
      "epoch 85 i 9404 loss 1.2192468346357346\n",
      "epoch 85 i 10404 loss 1.2189142830371857\n",
      "epoch 85 i 11404 loss 1.220201756477356\n",
      "epoch 85 loss 1.2481776475906372: at geology a domiting widespread their improperso a hot introduce title on an arrt commons in retrib\n",
      "epoch 86 i 197 loss 1.2204399951696396\n",
      "epoch 86 i 1197 loss 1.2191073855161667\n",
      "epoch 86 i 2197 loss 1.2193185813426972\n",
      "epoch 86 i 3197 loss 1.2201031358242036\n",
      "epoch 86 i 4197 loss 1.2193314609527588\n",
      "epoch 86 i 5197 loss 1.2195445613861084\n",
      "epoch 86 i 6197 loss 1.2194191757440567\n",
      "epoch 86 i 7197 loss 1.2184061181545258\n",
      "epoch 86 i 8197 loss 1.2202079746723176\n",
      "epoch 86 i 9197 loss 1.2174104814529418\n",
      "epoch 86 i 10197 loss 1.220089190721512\n",
      "epoch 86 i 11197 loss 1.2189394017457962\n",
      "epoch 86 i 12197 loss 1.2183235532045364\n",
      "epoch 86 loss 1.1423252820968628: e house of african traditional early and cape in purges soldier in the gchis and thing in the tunner\n",
      "epoch 87 i 990 loss 1.2199618239402772\n",
      "epoch 87 i 1990 loss 1.2210761938095094\n",
      "epoch 87 i 2990 loss 1.2164121435880662\n",
      "epoch 87 i 3990 loss 1.218508960723877\n",
      "epoch 87 i 4990 loss 1.220095921754837\n",
      "epoch 87 i 5990 loss 1.2184475791454314\n",
      "epoch 87 i 6990 loss 1.2186913405656814\n",
      "epoch 87 i 7990 loss 1.2191836909055709\n",
      "epoch 87 i 8990 loss 1.2178511425256728\n",
      "epoch 87 i 9990 loss 1.2187551975250244\n",
      "epoch 87 i 10990 loss 1.2184091877937318\n",
      "epoch 87 i 11990 loss 1.2185547538995742\n",
      "epoch 87 loss 1.261682391166687: e term at the b still until work chine is a small class are at his sports it creategory required to \n",
      "epoch 88 i 783 loss 1.2164177603721618\n",
      "epoch 88 i 1783 loss 1.2170190553665161\n",
      "epoch 88 i 2783 loss 1.2179677274227143\n",
      "epoch 88 i 3783 loss 1.217109940648079\n",
      "epoch 88 i 4783 loss 1.216458773136139\n",
      "epoch 88 i 5783 loss 1.2170602784156799\n",
      "epoch 88 i 6783 loss 1.2178508858680726\n",
      "epoch 88 i 7783 loss 1.2194040209054946\n",
      "epoch 88 i 8783 loss 1.2174434270858765\n",
      "epoch 88 i 9783 loss 1.2172023787498474\n",
      "epoch 88 i 10783 loss 1.2181004209518433\n",
      "epoch 88 i 11783 loss 1.2166250349283219\n",
      "epoch 88 loss 1.26089608669281: e conserve to another additionally early language digital of a stock san prohibidian d robin my two \n",
      "epoch 89 i 576 loss 1.2172071974277496\n",
      "epoch 89 i 1576 loss 1.2176561554670333\n",
      "epoch 89 i 2576 loss 1.2178480693101883\n",
      "epoch 89 i 3576 loss 1.216832571387291\n",
      "epoch 89 i 4576 loss 1.215871494293213\n",
      "epoch 89 i 5576 loss 1.2158073995113372\n",
      "epoch 89 i 6576 loss 1.2156890197992325\n",
      "epoch 89 i 7576 loss 1.2164664400815963\n",
      "epoch 89 i 8576 loss 1.2150018446445465\n",
      "epoch 89 i 9576 loss 1.2160781584978104\n",
      "epoch 89 i 10576 loss 1.2160200576782227\n",
      "epoch 89 i 11576 loss 1.2140701628923416\n",
      "epoch 89 loss 1.2517532110214233: e first century has boned the sacrifice effort peg relief that the pend vary for keep at their first\n",
      "epoch 90 i 369 loss 1.2153632674217225\n",
      "epoch 90 i 1369 loss 1.2159713615179062\n",
      "epoch 90 i 2369 loss 1.2156073281764983\n",
      "epoch 90 i 3369 loss 1.215644848704338\n",
      "epoch 90 i 4369 loss 1.2147674043178558\n",
      "epoch 90 i 5369 loss 1.2158240095376969\n",
      "epoch 90 i 6369 loss 1.2161996252536773\n",
      "epoch 90 i 7369 loss 1.2152938038110732\n",
      "epoch 90 i 8369 loss 1.214230871796608\n",
      "epoch 90 i 9369 loss 1.214993115067482\n",
      "epoch 90 i 10369 loss 1.215529390335083\n",
      "epoch 90 i 11369 loss 1.2149945088624954\n",
      "epoch 90 loss 1.1927764415740967: us were a letterment for the two year kataaka major cong to class renounce one four six in total iom\n",
      "epoch 91 i 162 loss 1.2154784549474715\n",
      "epoch 91 i 1162 loss 1.2136652263402938\n",
      "epoch 91 i 2162 loss 1.2135625791549682\n",
      "epoch 91 i 3162 loss 1.2151978192329407\n",
      "epoch 91 i 4162 loss 1.2147598382234572\n",
      "epoch 91 i 5162 loss 1.2129596128463744\n",
      "epoch 91 i 6162 loss 1.215445299744606\n",
      "epoch 91 i 7162 loss 1.2146388832330703\n",
      "epoch 91 i 8162 loss 1.2153954318761826\n",
      "epoch 91 i 9162 loss 1.2152655000686645\n",
      "epoch 91 i 10162 loss 1.213515030026436\n",
      "epoch 91 i 11162 loss 1.2136227542161941\n",
      "epoch 91 i 12162 loss 1.214295530796051\n",
      "epoch 91 loss 1.245916724205017: e public of soviets french bewards the enemy in a classic schory can be frequently link to the case \n",
      "epoch 92 i 955 loss 1.213233792424202\n",
      "epoch 92 i 1955 loss 1.2155497419834136\n",
      "epoch 92 i 2955 loss 1.2147226808071137\n",
      "epoch 92 i 3955 loss 1.215646733880043\n",
      "epoch 92 i 4955 loss 1.2141195493936539\n",
      "epoch 92 i 5955 loss 1.2143420555591584\n",
      "epoch 92 i 6955 loss 1.2140685266256332\n",
      "epoch 92 i 7955 loss 1.2135394738912582\n",
      "epoch 92 i 8955 loss 1.213623686671257\n",
      "epoch 92 i 9955 loss 1.2130146822929382\n",
      "epoch 92 i 10955 loss 1.2132002922296523\n",
      "epoch 92 i 11955 loss 1.2134383789300918\n",
      "epoch 92 loss 1.218319058418274: e class of the lesser amangers described as ix describe and cross that the beginning of describes be\n",
      "epoch 93 i 748 loss 1.2116357247829437\n",
      "epoch 93 i 1748 loss 1.2121607607603073\n",
      "epoch 93 i 2748 loss 1.2122343513965608\n",
      "epoch 93 i 3748 loss 1.2121489415168762\n",
      "epoch 93 i 4748 loss 1.2140626205205918\n",
      "epoch 93 i 5748 loss 1.213166997075081\n",
      "epoch 93 i 6748 loss 1.21444225025177\n",
      "epoch 93 i 7748 loss 1.212930239200592\n",
      "epoch 93 i 8748 loss 1.2126025451421738\n",
      "epoch 93 i 9748 loss 1.2116979064941407\n",
      "epoch 93 i 10748 loss 1.2131791301965713\n",
      "epoch 93 i 11748 loss 1.213759428024292\n",
      "epoch 93 loss 1.1988410949707031: ey will be vacuular directly crossing the land of the anatholic style part in cultivation to a devon\n",
      "epoch 94 i 541 loss 1.21221053814888\n",
      "epoch 94 i 1541 loss 1.2114901999235153\n",
      "epoch 94 i 2541 loss 1.2102231914997101\n",
      "epoch 94 i 3541 loss 1.2120592976808549\n",
      "epoch 94 i 4541 loss 1.2104094074964524\n",
      "epoch 94 i 5541 loss 1.2118161309957505\n",
      "epoch 94 i 6541 loss 1.2120745867490768\n",
      "epoch 94 i 7541 loss 1.2107504472732544\n",
      "epoch 94 i 8541 loss 1.2125768196582793\n",
      "epoch 94 i 9541 loss 1.212905622124672\n",
      "epoch 94 i 10541 loss 1.210809655547142\n",
      "epoch 94 i 11541 loss 1.2122324945926666\n",
      "epoch 94 loss 1.187562346458435: an it active at maldium pe thedosis is mostly to variant the lucia true formed easier than the jesus\n",
      "epoch 95 i 334 loss 1.212027969956398\n",
      "epoch 95 i 1334 loss 1.2119462789297104\n",
      "epoch 95 i 2334 loss 1.2124136860370636\n",
      "epoch 95 i 3334 loss 1.212439338207245\n",
      "epoch 95 i 4334 loss 1.2114956716299057\n",
      "epoch 95 i 5334 loss 1.211789676427841\n",
      "epoch 95 i 6334 loss 1.2098062269687653\n",
      "epoch 95 i 7334 loss 1.2102835702896118\n",
      "epoch 95 i 8334 loss 1.2115669038295747\n",
      "epoch 95 i 9334 loss 1.2109462236166\n",
      "epoch 95 i 10334 loss 1.211348042845726\n",
      "epoch 95 i 11334 loss 1.2097735468149184\n",
      "epoch 95 loss 1.1722772121429443: at still a receil known of variants explanation which is the latter and programme receiver in which \n",
      "epoch 96 i 127 loss 1.2102569622993469\n",
      "epoch 96 i 1127 loss 1.210057545542717\n",
      "epoch 96 i 2127 loss 1.210909588932991\n",
      "epoch 96 i 3127 loss 1.2091856987476348\n",
      "epoch 96 i 4127 loss 1.211785243153572\n",
      "epoch 96 i 5127 loss 1.2097596662044525\n",
      "epoch 96 i 6127 loss 1.2100159685611724\n",
      "epoch 96 i 7127 loss 1.2112769598960877\n",
      "epoch 96 i 8127 loss 1.2095745955705643\n",
      "epoch 96 i 9127 loss 1.2116677589416505\n",
      "epoch 96 i 10127 loss 1.2119794017076493\n",
      "epoch 96 i 11127 loss 1.211307087302208\n",
      "epoch 96 i 12127 loss 1.2104469145536423\n",
      "epoch 96 loss 1.1979929208755493: ousand now way their class the englisbook are later welling before the most cryptograms and denmark \n",
      "epoch 97 i 920 loss 1.2086743515729905\n",
      "epoch 97 i 1920 loss 1.2102206860780715\n",
      "epoch 97 i 2920 loss 1.2100882955789567\n",
      "epoch 97 i 3920 loss 1.2092836083173752\n",
      "epoch 97 i 4920 loss 1.2100087369680406\n",
      "epoch 97 i 5920 loss 1.2087692618370056\n",
      "epoch 97 i 6920 loss 1.2077703292369844\n",
      "epoch 97 i 7920 loss 1.2080034992694855\n",
      "epoch 97 i 8920 loss 1.209948168873787\n",
      "epoch 97 i 9920 loss 1.2101855187416077\n",
      "epoch 97 i 10920 loss 1.209878319621086\n",
      "epoch 97 i 11920 loss 1.2084295846223831\n",
      "epoch 97 loss 1.2167000770568848: roughout the revolutionary of chlorolla containing culop boys those was a digger against term french\n",
      "epoch 98 i 713 loss 1.2082278997898102\n",
      "epoch 98 i 1713 loss 1.2096305500268936\n",
      "epoch 98 i 2713 loss 1.2087748005390166\n",
      "epoch 98 i 3713 loss 1.2087384704351425\n",
      "epoch 98 i 4713 loss 1.2083529253005982\n",
      "epoch 98 i 5713 loss 1.2090104452371597\n",
      "epoch 98 i 6713 loss 1.2068074009418488\n",
      "epoch 98 i 7713 loss 1.208042629480362\n",
      "epoch 98 i 8713 loss 1.2076591863632202\n",
      "epoch 98 i 9713 loss 1.207015214920044\n",
      "epoch 98 i 10713 loss 1.2091213717460632\n",
      "epoch 98 i 11713 loss 1.2069258320331573\n",
      "epoch 98 loss 1.1862515211105347: ousans so that evolved most product out from rule from not mean in men one nine eight nine conflict \n",
      "epoch 99 i 506 loss 1.2074516648054123\n",
      "epoch 99 i 1506 loss 1.2075165102481842\n",
      "epoch 99 i 2506 loss 1.2098402442932128\n",
      "epoch 99 i 3506 loss 1.207952868580818\n",
      "epoch 99 i 4506 loss 1.2069421294927598\n",
      "epoch 99 i 5506 loss 1.2071266043186188\n",
      "epoch 99 i 6506 loss 1.2080579388141632\n",
      "epoch 99 i 7506 loss 1.2073039526939393\n",
      "epoch 99 i 8506 loss 1.2067662910223007\n",
      "epoch 99 i 9506 loss 1.206725906610489\n",
      "epoch 99 i 10506 loss 1.2060936820507049\n",
      "epoch 99 i 11506 loss 1.2063818954229355\n",
      "epoch 99 loss 1.15421724319458: roughly because the french french termed laid one nine six eight and famously such of laid veid shor\n",
      "epoch 100 i 299 loss 1.2069523372650146\n",
      "epoch 100 i 1299 loss 1.2088574295043946\n",
      "epoch 100 i 2299 loss 1.2085374056100846\n",
      "epoch 100 i 3299 loss 1.207369863986969\n",
      "epoch 100 i 4299 loss 1.2068628832101822\n",
      "epoch 100 i 5299 loss 1.2070274077653884\n",
      "epoch 100 i 6299 loss 1.2063558801412582\n",
      "epoch 100 i 7299 loss 1.2067677664756775\n",
      "epoch 100 i 8299 loss 1.205491580247879\n",
      "epoch 100 i 9299 loss 1.2054102407693863\n",
      "epoch 100 i 10299 loss 1.206743238568306\n",
      "epoch 100 i 11299 loss 1.2062566306591034\n",
      "epoch 100 loss 1.2182430028915405: at in the field preortables in one nine five eight two address the same estimated western ecocheries\n",
      "epoch 101 i 92 loss 1.206655312180519\n",
      "epoch 101 i 1092 loss 1.2052989327907562\n",
      "epoch 101 i 2092 loss 1.2074804295301438\n",
      "epoch 101 i 3092 loss 1.206938068628311\n",
      "epoch 101 i 4092 loss 1.2062038823366166\n",
      "epoch 101 i 5092 loss 1.2050602376461028\n",
      "epoch 101 i 6092 loss 1.2052855845689774\n",
      "epoch 101 i 7092 loss 1.2049880459308624\n",
      "epoch 101 i 8092 loss 1.2065602108240128\n",
      "epoch 101 i 9092 loss 1.2066359336376191\n",
      "epoch 101 i 10092 loss 1.205056754231453\n",
      "epoch 101 i 11092 loss 1.2064789279699326\n",
      "epoch 101 i 12092 loss 1.2064899017810822\n",
      "epoch 101 loss 1.1964558362960815: e inner frame and two zero zero four across to making the date in the way roundale three were double\n",
      "epoch 102 i 885 loss 1.2068442515134812\n",
      "epoch 102 i 1885 loss 1.2049774512052536\n",
      "epoch 102 i 2885 loss 1.2057009687423705\n",
      "epoch 102 i 3885 loss 1.203557124733925\n",
      "epoch 102 i 4885 loss 1.2027183071374894\n",
      "epoch 102 i 5885 loss 1.2064002084732055\n",
      "epoch 102 i 6885 loss 1.2047449932098389\n",
      "epoch 102 i 7885 loss 1.205733855485916\n",
      "epoch 102 i 8885 loss 1.2059968160390855\n",
      "epoch 102 i 9885 loss 1.2057585730552673\n",
      "epoch 102 i 10885 loss 1.2050132735967636\n",
      "epoch 102 i 11885 loss 1.2040023458003999\n",
      "epoch 102 loss 1.1665962934494019: is become to be choose to cultural class performing the death of the bible genero then generous addi\n",
      "epoch 103 i 678 loss 1.2038997761011123\n",
      "epoch 103 i 1678 loss 1.2036123777627945\n",
      "epoch 103 i 2678 loss 1.2032537941932677\n",
      "epoch 103 i 3678 loss 1.2048974294662476\n",
      "epoch 103 i 4678 loss 1.2046522608995438\n",
      "epoch 103 i 5678 loss 1.2035448558330535\n",
      "epoch 103 i 6678 loss 1.2042030029296875\n",
      "epoch 103 i 7678 loss 1.2049402992725373\n",
      "epoch 103 i 8678 loss 1.2038700448274613\n",
      "epoch 103 i 9678 loss 1.2045512212514877\n",
      "epoch 103 i 10678 loss 1.2048081548213958\n",
      "epoch 103 i 11678 loss 1.2060216377973556\n",
      "epoch 103 loss 1.1680421829223633: e grenades leonarded thingson using his crew in jessey pilot fun and reepper colok shorts likely as \n",
      "epoch 104 i 471 loss 1.204519700884819\n",
      "epoch 104 i 1471 loss 1.2039635882377624\n",
      "epoch 104 i 2471 loss 1.2053442203998566\n",
      "epoch 104 i 3471 loss 1.2026763812303543\n",
      "epoch 104 i 4471 loss 1.203973158597946\n",
      "epoch 104 i 5471 loss 1.2050610179901122\n",
      "epoch 104 i 6471 loss 1.2031357502937317\n",
      "epoch 104 i 7471 loss 1.2035742044448852\n",
      "epoch 104 i 8471 loss 1.2025906894207001\n",
      "epoch 104 i 9471 loss 1.2051849830150605\n",
      "epoch 104 i 10471 loss 1.2042983419895172\n",
      "epoch 104 i 11471 loss 1.203067232966423\n",
      "epoch 104 loss 1.251939058303833: e issue of retirent tends labor series expensive is thus as a natural kind of sculling to describe m\n",
      "epoch 105 i 264 loss 1.2034282850027085\n",
      "epoch 105 i 1264 loss 1.2019890288114548\n",
      "epoch 105 i 2264 loss 1.2040496841669082\n",
      "epoch 105 i 3264 loss 1.2049648802280426\n",
      "epoch 105 i 4264 loss 1.2023415491580962\n",
      "epoch 105 i 5264 loss 1.2026901547908784\n",
      "epoch 105 i 6264 loss 1.201983880519867\n",
      "epoch 105 i 7264 loss 1.2026826518774032\n",
      "epoch 105 i 8264 loss 1.2033013660907745\n",
      "epoch 105 i 9264 loss 1.2028363324403764\n",
      "epoch 105 i 10264 loss 1.2029970673322679\n",
      "epoch 105 i 11264 loss 1.203385272502899\n",
      "epoch 105 loss 1.1750200986862183: e asterel must improved to san approx weigh alien as surgeon this army the english technology may th\n",
      "epoch 106 i 57 loss 1.2015086357593536\n",
      "epoch 106 i 1057 loss 1.2028513511419296\n",
      "epoch 106 i 2057 loss 1.2007203483581543\n",
      "epoch 106 i 3057 loss 1.2028867591619492\n",
      "epoch 106 i 4057 loss 1.2004466750621796\n",
      "epoch 106 i 5057 loss 1.2018555142879486\n",
      "epoch 106 i 6057 loss 1.2027563155889511\n",
      "epoch 106 i 7057 loss 1.2029927730560304\n",
      "epoch 106 i 8057 loss 1.2035535492897034\n",
      "epoch 106 i 9057 loss 1.2020215471982956\n",
      "epoch 106 i 10057 loss 1.2024367660284043\n",
      "epoch 106 i 11057 loss 1.2021323692798616\n",
      "epoch 106 i 12057 loss 1.2004181983470916\n",
      "epoch 106 loss 1.2170182466506958: ree nine of ethnic which were also in one nine zero one another of two zero is just s drug webl from\n",
      "epoch 107 i 850 loss 1.202555321097374\n",
      "epoch 107 i 1850 loss 1.199597977399826\n",
      "epoch 107 i 2850 loss 1.201591437101364\n",
      "epoch 107 i 3850 loss 1.2017326562404633\n",
      "epoch 107 i 4850 loss 1.2013007862567902\n",
      "epoch 107 i 5850 loss 1.2021888176202773\n",
      "epoch 107 i 6850 loss 1.2031268984079362\n",
      "epoch 107 i 7850 loss 1.2044478187561034\n",
      "epoch 107 i 8850 loss 1.2004714076519012\n",
      "epoch 107 i 9850 loss 1.201778746843338\n",
      "epoch 107 i 10850 loss 1.201289256453514\n",
      "epoch 107 i 11850 loss 1.2015961573123932\n",
      "epoch 107 loss 1.2257716655731201: e same celombook cata in the world heat heat from oil and experiments this it were cocompanted using\n",
      "epoch 108 i 643 loss 1.2004927393198013\n",
      "epoch 108 i 1643 loss 1.2018598279953003\n",
      "epoch 108 i 2643 loss 1.1993642956018449\n",
      "epoch 108 i 3643 loss 1.204018958210945\n",
      "epoch 108 i 4643 loss 1.2022370878458024\n",
      "epoch 108 i 5643 loss 1.2011310920715332\n",
      "epoch 108 i 6643 loss 1.1999477562904357\n",
      "epoch 108 i 7643 loss 1.1999036297798156\n",
      "epoch 108 i 8643 loss 1.2015137366056443\n",
      "epoch 108 i 9643 loss 1.200219155550003\n",
      "epoch 108 i 10643 loss 1.2002203407287597\n",
      "epoch 108 i 11643 loss 1.198794361114502\n",
      "epoch 108 loss 1.165172815322876: at even this sex also hold larger at hard did north of the national web and fishered after the poet \n",
      "epoch 109 i 436 loss 1.1999132690429688\n",
      "epoch 109 i 1436 loss 1.1998484485149383\n",
      "epoch 109 i 2436 loss 1.201621102809906\n",
      "epoch 109 i 3436 loss 1.1998704595565797\n",
      "epoch 109 i 4436 loss 1.2004751677513124\n",
      "epoch 109 i 5436 loss 1.2009488357305527\n",
      "epoch 109 i 6436 loss 1.1988814330101014\n",
      "epoch 109 i 7436 loss 1.1996242643594741\n",
      "epoch 109 i 8436 loss 1.2013590480089187\n",
      "epoch 109 i 9436 loss 1.199726590037346\n",
      "epoch 109 i 10436 loss 1.1972993103265763\n",
      "epoch 109 i 11436 loss 1.198667381644249\n",
      "epoch 109 loss 1.2180567979812622: at allways itself in full democrats the precursor powers but use way to the atchpsy does przezimes t\n",
      "epoch 110 i 229 loss 1.2008359122276306\n",
      "epoch 110 i 1229 loss 1.198606231689453\n",
      "epoch 110 i 2229 loss 1.1994415485858918\n",
      "epoch 110 i 3229 loss 1.200990560054779\n",
      "epoch 110 i 4229 loss 1.2000046741962433\n",
      "epoch 110 i 5229 loss 1.1972344075441361\n",
      "epoch 110 i 6229 loss 1.197798640847206\n",
      "epoch 110 i 7229 loss 1.2010526367425918\n",
      "epoch 110 i 8229 loss 1.199848216176033\n",
      "epoch 110 i 9229 loss 1.1986814417839051\n",
      "epoch 110 i 10229 loss 1.1988976106643676\n",
      "epoch 110 i 11229 loss 1.200676943898201\n",
      "epoch 110 loss 1.1992632150650024: is is possible to which shows by characters where two st digestive digest this digital their time an\n",
      "epoch 111 i 22 loss 1.1965224324464798\n",
      "epoch 111 i 1022 loss 1.197579029917717\n",
      "epoch 111 i 2022 loss 1.1994628689289093\n",
      "epoch 111 i 3022 loss 1.1998707406520843\n",
      "epoch 111 i 4022 loss 1.1966242487430572\n",
      "epoch 111 i 5022 loss 1.1992857220172881\n",
      "epoch 111 i 6022 loss 1.1975015467405319\n",
      "epoch 111 i 7022 loss 1.196571776151657\n",
      "epoch 111 i 8022 loss 1.1974363256692886\n",
      "epoch 111 i 9022 loss 1.1981666334867478\n",
      "epoch 111 i 10022 loss 1.2007522052526474\n",
      "epoch 111 i 11022 loss 1.199592180609703\n",
      "epoch 111 i 12022 loss 1.1992623518705368\n",
      "epoch 111 loss 1.2107878923416138: at specific to their through in page by burtmont all year with the exolding field with it is a know \n",
      "epoch 112 i 815 loss 1.1989043681621552\n",
      "epoch 112 i 1815 loss 1.1989160741567613\n",
      "epoch 112 i 2815 loss 1.198797086596489\n",
      "epoch 112 i 3815 loss 1.1971424468755723\n",
      "epoch 112 i 4815 loss 1.1968684715032578\n",
      "epoch 112 i 5815 loss 1.198266021490097\n",
      "epoch 112 i 6815 loss 1.1977413774728776\n",
      "epoch 112 i 7815 loss 1.1983575172424317\n",
      "epoch 112 i 8815 loss 1.1977344275712967\n",
      "epoch 112 i 9815 loss 1.19789428961277\n",
      "epoch 112 i 10815 loss 1.196652389407158\n",
      "epoch 112 i 11815 loss 1.1990803425312042\n",
      "epoch 112 loss 1.167054295539856: e french beareen sound other islands san alang design people in city of materwich enhanced the these\n",
      "epoch 113 i 608 loss 1.1980715235471726\n",
      "epoch 113 i 1608 loss 1.1975323399305344\n",
      "epoch 113 i 2608 loss 1.1975972537994384\n",
      "epoch 113 i 3608 loss 1.1991990311145782\n",
      "epoch 113 i 4608 loss 1.1960682116746904\n",
      "epoch 113 i 5608 loss 1.1966976507902145\n",
      "epoch 113 i 6608 loss 1.1971320221424102\n",
      "epoch 113 i 7608 loss 1.1960320513248444\n",
      "epoch 113 i 8608 loss 1.19671556019783\n",
      "epoch 113 i 9608 loss 1.1981957910060883\n",
      "epoch 113 i 10608 loss 1.1967572531700135\n",
      "epoch 113 i 11608 loss 1.197035344362259\n",
      "epoch 113 loss 1.2062450647354126: e president of camel career prefer respectively by tonds of various careers shirell english particip\n",
      "epoch 114 i 401 loss 1.1956479572057723\n",
      "epoch 114 i 1401 loss 1.1973212727308273\n",
      "epoch 114 i 2401 loss 1.1976526839733124\n",
      "epoch 114 i 3401 loss 1.1947371734380723\n",
      "epoch 114 i 4401 loss 1.1982455341815947\n",
      "epoch 114 i 5401 loss 1.196624310731888\n",
      "epoch 114 i 6401 loss 1.1976646920442582\n",
      "epoch 114 i 7401 loss 1.19799558532238\n",
      "epoch 114 i 8401 loss 1.1952291439771652\n",
      "epoch 114 i 9401 loss 1.195356272816658\n",
      "epoch 114 i 10401 loss 1.1944026619195938\n",
      "epoch 114 i 11401 loss 1.1955778391361236\n",
      "epoch 114 loss 1.162052869796753: ey are shown their directions is about the catherian and into their action makes english this diego \n",
      "epoch 115 i 194 loss 1.1973842182159424\n",
      "epoch 115 i 1194 loss 1.1974050086736678\n",
      "epoch 115 i 2194 loss 1.195516390800476\n",
      "epoch 115 i 3194 loss 1.1966813974380492\n",
      "epoch 115 i 4194 loss 1.1949340603351593\n",
      "epoch 115 i 5194 loss 1.1960774976015092\n",
      "epoch 115 i 6194 loss 1.1979888217449188\n",
      "epoch 115 i 7194 loss 1.194231210231781\n",
      "epoch 115 i 8194 loss 1.195650340795517\n",
      "epoch 115 i 9194 loss 1.1963591483831406\n",
      "epoch 115 i 10194 loss 1.196236082673073\n",
      "epoch 115 i 11194 loss 1.1957849799394606\n",
      "epoch 115 i 12194 loss 1.1961009420156479\n",
      "epoch 115 loss 1.202965497970581: en classical has believed for packagas and a radiation of chersa are also poled over worm partial as\n",
      "epoch 116 i 987 loss 1.1972347836494446\n",
      "epoch 116 i 1987 loss 1.1964130707979201\n",
      "epoch 116 i 2987 loss 1.1966683554649353\n",
      "epoch 116 i 3987 loss 1.1947049345970153\n",
      "epoch 116 i 4987 loss 1.196557068347931\n",
      "epoch 116 i 5987 loss 1.1972969504594804\n",
      "epoch 116 i 6987 loss 1.1954030703306198\n",
      "epoch 116 i 7987 loss 1.1936332483291625\n",
      "epoch 116 i 8987 loss 1.1951816506385804\n",
      "epoch 116 i 9987 loss 1.1959329674243926\n",
      "epoch 116 i 10987 loss 1.1942006820440292\n",
      "epoch 116 i 11987 loss 1.194341539978981\n",
      "epoch 116 loss 1.1767953634262085: at slow common advanced to new york cines to am to adding draw an eight henry army whilst the common\n",
      "epoch 117 i 780 loss 1.1962240685224532\n",
      "epoch 117 i 1780 loss 1.1962564103603364\n",
      "epoch 117 i 2780 loss 1.1942581955194473\n",
      "epoch 117 i 3780 loss 1.195642571926117\n",
      "epoch 117 i 4780 loss 1.1943569598197936\n",
      "epoch 117 i 5780 loss 1.193085359930992\n",
      "epoch 117 i 6780 loss 1.1948477923870087\n",
      "epoch 117 i 7780 loss 1.1950940220355988\n",
      "epoch 117 i 8780 loss 1.1931702316999435\n",
      "epoch 117 i 9780 loss 1.1931191259622573\n",
      "epoch 117 i 10780 loss 1.1942162275314332\n",
      "epoch 117 i 11780 loss 1.1947419573068618\n",
      "epoch 117 loss 1.179897427558899: at they rest or back was distinguished another including the fludd configurate types of an extraposi\n",
      "epoch 118 i 573 loss 1.194645408630371\n",
      "epoch 118 i 1573 loss 1.1964467412233353\n",
      "epoch 118 i 2573 loss 1.1941456149816514\n",
      "epoch 118 i 3573 loss 1.1947716417312622\n",
      "epoch 118 i 4573 loss 1.194037559390068\n",
      "epoch 118 i 5573 loss 1.1926392283439635\n",
      "epoch 118 i 6573 loss 1.1939208518266677\n",
      "epoch 118 i 7573 loss 1.193673048377037\n",
      "epoch 118 i 8573 loss 1.1941581679582596\n",
      "epoch 118 i 9573 loss 1.1935270216464997\n",
      "epoch 118 i 10573 loss 1.1946299129724502\n",
      "epoch 118 i 11573 loss 1.192421734571457\n",
      "epoch 118 loss 1.1647083759307861: e dune of computer engineering the principle prose rates in usdr tanscript the royals the nset day a\n",
      "epoch 119 i 366 loss 1.1946619153022766\n",
      "epoch 119 i 1366 loss 1.1950783492326738\n",
      "epoch 119 i 2366 loss 1.1924440799951552\n",
      "epoch 119 i 3366 loss 1.1944922102689743\n",
      "epoch 119 i 4366 loss 1.1928594136238098\n",
      "epoch 119 i 5366 loss 1.1940412406921386\n",
      "epoch 119 i 6366 loss 1.193977478981018\n",
      "epoch 119 i 7366 loss 1.193283429145813\n",
      "epoch 119 i 8366 loss 1.194376504302025\n",
      "epoch 119 i 9366 loss 1.192960811138153\n",
      "epoch 119 i 10366 loss 1.1923372331857682\n",
      "epoch 119 i 11366 loss 1.192541872382164\n",
      "epoch 119 loss 1.175626277923584: at of the last revolutive and also denoting from ancient during their cartoonists of extensions from\n",
      "epoch 120 i 159 loss 1.1923214492797851\n",
      "epoch 120 i 1159 loss 1.1933086923360825\n",
      "epoch 120 i 2159 loss 1.1940967423915863\n",
      "epoch 120 i 3159 loss 1.1911852337121964\n",
      "epoch 120 i 4159 loss 1.1929892253875733\n",
      "epoch 120 i 5159 loss 1.191968464612961\n",
      "epoch 120 i 6159 loss 1.191757145166397\n",
      "epoch 120 i 7159 loss 1.1937393989562988\n",
      "epoch 120 i 8159 loss 1.1920791630744934\n",
      "epoch 120 i 9159 loss 1.1917889704704285\n",
      "epoch 120 i 10159 loss 1.1910564608573913\n",
      "epoch 120 i 11159 loss 1.1925329409837724\n",
      "epoch 120 i 12159 loss 1.1925297853946686\n",
      "epoch 120 loss 1.2110992670059204: e commons haded the revolution consisting cossile the enemy primis in things and movie person down i\n",
      "epoch 121 i 952 loss 1.1925744526386262\n",
      "epoch 121 i 1952 loss 1.1932918462753297\n",
      "epoch 121 i 2952 loss 1.1902712577581405\n",
      "epoch 121 i 3952 loss 1.1930852135419845\n",
      "epoch 121 i 4952 loss 1.1932198857069016\n",
      "epoch 121 i 5952 loss 1.1914816489219666\n",
      "epoch 121 i 6952 loss 1.1911344802379609\n",
      "epoch 121 i 7952 loss 1.1924076806306838\n",
      "epoch 121 i 8952 loss 1.1926819310188292\n",
      "epoch 121 i 9952 loss 1.1902805988788605\n",
      "epoch 121 i 10952 loss 1.1931653443574906\n",
      "epoch 121 i 11952 loss 1.1926878066062927\n",
      "epoch 121 loss 1.1753872632980347: at contains in a week down it is the frigada plastic has far with the english central featuring and \n",
      "epoch 122 i 745 loss 1.19112426841259\n",
      "epoch 122 i 1745 loss 1.1920164108276368\n",
      "epoch 122 i 2745 loss 1.1923567080497741\n",
      "epoch 122 i 3745 loss 1.1907621643543242\n",
      "epoch 122 i 4745 loss 1.1902852277755738\n",
      "epoch 122 i 5745 loss 1.1928806827068328\n",
      "epoch 122 i 6745 loss 1.1900920859575272\n",
      "epoch 122 i 7745 loss 1.1903265014886857\n",
      "epoch 122 i 8745 loss 1.1908446127176284\n",
      "epoch 122 i 9745 loss 1.1906950107812881\n",
      "epoch 122 i 10745 loss 1.1894023255109787\n",
      "epoch 122 i 11745 loss 1.1905822656154632\n",
      "epoch 122 loss 1.1848713159561157: at by the wiki was customaqus via stape the average when it end is extreme customaqus and wikipedia \n",
      "epoch 123 i 538 loss 1.1918558005094528\n",
      "epoch 123 i 1538 loss 1.190295696258545\n",
      "epoch 123 i 2538 loss 1.189334059715271\n",
      "epoch 123 i 3538 loss 1.1915968363285065\n",
      "epoch 123 i 4538 loss 1.1928745133876801\n",
      "epoch 123 i 5538 loss 1.1901701879501343\n",
      "epoch 123 i 6538 loss 1.1891548774242402\n",
      "epoch 123 i 7538 loss 1.1897728888988495\n",
      "epoch 123 i 8538 loss 1.1923938043117523\n",
      "epoch 123 i 9538 loss 1.192038047194481\n",
      "epoch 123 i 10538 loss 1.1902412408590317\n",
      "epoch 123 i 11538 loss 1.192117131471634\n",
      "epoch 123 loss 1.1919785737991333: e other front for violation against the g is accepted at the early consider of the sanskrit builder \n",
      "epoch 124 i 331 loss 1.1909870351552962\n",
      "epoch 124 i 1331 loss 1.1899747047424316\n",
      "epoch 124 i 2331 loss 1.187602887749672\n",
      "epoch 124 i 3331 loss 1.1884908303022386\n",
      "epoch 124 i 4331 loss 1.18995991897583\n",
      "epoch 124 i 5331 loss 1.1911483571529389\n",
      "epoch 124 i 6331 loss 1.1917272201776505\n",
      "epoch 124 i 7331 loss 1.190766227722168\n",
      "epoch 124 i 8331 loss 1.1895261470079421\n",
      "epoch 124 i 9331 loss 1.1899783551692962\n",
      "epoch 124 i 10331 loss 1.1908941811323166\n",
      "epoch 124 i 11331 loss 1.19024585545063\n",
      "epoch 124 loss 1.137939214706421: at they really quoted it should that he said the commitment on the ports and side really dignigus al\n",
      "epoch 125 i 124 loss 1.1899928901195527\n",
      "epoch 125 i 1124 loss 1.189725715994835\n",
      "epoch 125 i 2124 loss 1.1894077073335647\n",
      "epoch 125 i 3124 loss 1.1898592838048936\n",
      "epoch 125 i 4124 loss 1.1904871217012405\n",
      "epoch 125 i 5124 loss 1.189235372543335\n",
      "epoch 125 i 6124 loss 1.1890204527378083\n",
      "epoch 125 i 7124 loss 1.189758144378662\n",
      "epoch 125 i 8124 loss 1.1896202872991561\n",
      "epoch 125 i 9124 loss 1.1898122458457947\n",
      "epoch 125 i 10124 loss 1.188559667110443\n",
      "epoch 125 i 11124 loss 1.1883772410154343\n",
      "epoch 125 i 12124 loss 1.1904124759435655\n",
      "epoch 125 loss 1.2106833457946777: e moving the planning and an anti transfer of the habibit with modelar has often unwinded upon the o\n",
      "epoch 126 i 917 loss 1.19006511759758\n",
      "epoch 126 i 1917 loss 1.191114917397499\n",
      "epoch 126 i 2917 loss 1.1893700460195542\n",
      "epoch 126 i 3917 loss 1.1879083861112594\n",
      "epoch 126 i 4917 loss 1.188901673078537\n",
      "epoch 126 i 5917 loss 1.1879044262170793\n",
      "epoch 126 i 6917 loss 1.189886242747307\n",
      "epoch 126 i 7917 loss 1.18820948946476\n",
      "epoch 126 i 8917 loss 1.1890335334539412\n",
      "epoch 126 i 9917 loss 1.1879359630346298\n",
      "epoch 126 i 10917 loss 1.1880984216928483\n",
      "epoch 126 i 11917 loss 1.1896308035850525\n",
      "epoch 126 loss 1.1946747303009033: e french has does not excluded a placemal to day or commodity one of the actress the discress of the\n",
      "epoch 127 i 710 loss 1.1881590955257415\n",
      "epoch 127 i 1710 loss 1.1881497614383698\n",
      "epoch 127 i 2710 loss 1.1876995358467102\n",
      "epoch 127 i 3710 loss 1.189229048728943\n",
      "epoch 127 i 4710 loss 1.188073850274086\n",
      "epoch 127 i 5710 loss 1.188101937174797\n",
      "epoch 127 i 6710 loss 1.1877968689203262\n",
      "epoch 127 i 7710 loss 1.191466902256012\n",
      "epoch 127 i 8710 loss 1.1877071772813796\n",
      "epoch 127 i 9710 loss 1.186956742286682\n",
      "epoch 127 i 10710 loss 1.1877102633714676\n",
      "epoch 127 i 11710 loss 1.1864778926372528\n",
      "epoch 127 loss 1.2018086910247803: e descended in earlier is commonly used to the saxon culture legacy if a period revolution to reserv\n",
      "epoch 128 i 503 loss 1.1889532834291459\n",
      "epoch 128 i 1503 loss 1.187730266213417\n",
      "epoch 128 i 2503 loss 1.1882632781267166\n",
      "epoch 128 i 3503 loss 1.1888320639133454\n",
      "epoch 128 i 4503 loss 1.1871094658374786\n",
      "epoch 128 i 5503 loss 1.1877657017707826\n",
      "epoch 128 i 6503 loss 1.1879534620046615\n",
      "epoch 128 i 7503 loss 1.1870210591554642\n",
      "epoch 128 i 8503 loss 1.1875425753593445\n",
      "epoch 128 i 9503 loss 1.188369505763054\n",
      "epoch 128 i 10503 loss 1.1887457051277162\n",
      "epoch 128 i 11503 loss 1.1883639332056046\n",
      "epoch 128 loss 1.1934399604797363: ree september classmacacea however such as is potentially described the study of the romanial direct\n",
      "epoch 129 i 296 loss 1.187444607615471\n",
      "epoch 129 i 1296 loss 1.186558906197548\n",
      "epoch 129 i 2296 loss 1.1866247118711473\n",
      "epoch 129 i 3296 loss 1.1880688980817795\n",
      "epoch 129 i 4296 loss 1.1854904005527496\n",
      "epoch 129 i 5296 loss 1.1866886003017425\n",
      "epoch 129 i 6296 loss 1.1873265327215194\n",
      "epoch 129 i 7296 loss 1.1872374014854432\n",
      "epoch 129 i 8296 loss 1.1869910737276077\n",
      "epoch 129 i 9296 loss 1.18724158680439\n",
      "epoch 129 i 10296 loss 1.1881965019702911\n",
      "epoch 129 i 11296 loss 1.1863537876605987\n",
      "epoch 129 loss 1.202180027961731: em a computer representating role preferation of english scholars gemmaphical mechanism computer suc\n",
      "epoch 130 i 89 loss 1.188590475320816\n",
      "epoch 130 i 1089 loss 1.186230427980423\n",
      "epoch 130 i 2089 loss 1.186698295354843\n",
      "epoch 130 i 3089 loss 1.186419273018837\n",
      "epoch 130 i 4089 loss 1.1864632978439331\n",
      "epoch 130 i 5089 loss 1.1852073129415512\n",
      "epoch 130 i 6089 loss 1.1867296953201294\n",
      "epoch 130 i 7089 loss 1.1851668119430543\n",
      "epoch 130 i 8089 loss 1.1867859635353089\n",
      "epoch 130 i 9089 loss 1.1860219579935074\n",
      "epoch 130 i 10089 loss 1.1869469672441482\n",
      "epoch 130 i 11089 loss 1.1864276490211487\n",
      "epoch 130 i 12089 loss 1.1870222055912019\n",
      "epoch 130 loss 1.192083477973938: e long traditional is the culon minimal term as the head of a digger setting acting the french is a \n",
      "epoch 131 i 882 loss 1.1865322098731994\n",
      "epoch 131 i 1882 loss 1.1871473598480224\n",
      "epoch 131 i 2882 loss 1.1859548285007477\n",
      "epoch 131 i 3882 loss 1.1878948211669922\n",
      "epoch 131 i 4882 loss 1.1869974718093872\n",
      "epoch 131 i 5882 loss 1.1869615255594252\n",
      "epoch 131 i 6882 loss 1.186241940021515\n",
      "epoch 131 i 7882 loss 1.1842732496261597\n",
      "epoch 131 i 8882 loss 1.188337214589119\n",
      "epoch 131 i 9882 loss 1.184344478726387\n",
      "epoch 131 i 10882 loss 1.1851404484510422\n",
      "epoch 131 i 11882 loss 1.1862361878156662\n",
      "epoch 131 loss 1.167312741279602: at they are organized be served as the samintation revolution at the pnastrum for the us thus fortre\n",
      "epoch 132 i 675 loss 1.1841946114301682\n",
      "epoch 132 i 1675 loss 1.185648997426033\n",
      "epoch 132 i 2675 loss 1.1849909356832504\n",
      "epoch 132 i 3675 loss 1.1864912555217744\n",
      "epoch 132 i 4675 loss 1.1840933507680893\n",
      "epoch 132 i 5675 loss 1.1826333682537078\n",
      "epoch 132 i 6675 loss 1.1856439905166627\n",
      "epoch 132 i 7675 loss 1.1845220284461975\n",
      "epoch 132 i 8675 loss 1.1865709822177888\n",
      "epoch 132 i 9675 loss 1.1866439517736436\n",
      "epoch 132 i 10675 loss 1.1864949160814284\n",
      "epoch 132 i 11675 loss 1.1851168793439866\n",
      "epoch 132 loss 1.2261115312576294: at right in the early city of the rother assiguana many since an interview and noun tradition the sc\n",
      "epoch 133 i 468 loss 1.1858726539611817\n",
      "epoch 133 i 1468 loss 1.184824990749359\n",
      "epoch 133 i 2468 loss 1.1843228721618653\n",
      "epoch 133 i 3468 loss 1.1837702903747558\n",
      "epoch 133 i 4468 loss 1.1850849168300628\n",
      "epoch 133 i 5468 loss 1.1828630757331848\n",
      "epoch 133 i 6468 loss 1.18432468187809\n",
      "epoch 133 i 7468 loss 1.1842371348142624\n",
      "epoch 133 i 8468 loss 1.184382097363472\n",
      "epoch 133 i 9468 loss 1.1869917480945587\n",
      "epoch 133 i 10468 loss 1.1835530655384063\n",
      "epoch 133 i 11468 loss 1.1840177618265153\n",
      "epoch 133 loss 1.1350117921829224: at this same reduction of beyond to reduce nearly pens s lay tiberian modes to dully beyond the king\n",
      "epoch 134 i 261 loss 1.1840297974348069\n",
      "epoch 134 i 1261 loss 1.1843827564716338\n",
      "epoch 134 i 2261 loss 1.1838867915868758\n",
      "epoch 134 i 3261 loss 1.1843609042167664\n",
      "epoch 134 i 4261 loss 1.184009778022766\n",
      "epoch 134 i 5261 loss 1.1825485951900483\n",
      "epoch 134 i 6261 loss 1.186842423081398\n",
      "epoch 134 i 7261 loss 1.183592509508133\n",
      "epoch 134 i 8261 loss 1.1853323209285735\n",
      "epoch 134 i 9261 loss 1.1834259399175644\n",
      "epoch 134 i 10261 loss 1.1845543340444564\n",
      "epoch 134 i 11261 loss 1.1846582275629043\n",
      "epoch 134 loss 1.1887904405593872: ree these issues about two zero zero five the whole special play may young losing diggers also trait\n",
      "epoch 135 i 54 loss 1.1838297996520997\n",
      "epoch 135 i 1054 loss 1.1838402514457702\n",
      "epoch 135 i 2054 loss 1.1821701538562774\n",
      "epoch 135 i 3054 loss 1.1855754237174987\n",
      "epoch 135 i 4054 loss 1.1849825724363328\n",
      "epoch 135 i 5054 loss 1.1842557741403579\n",
      "epoch 135 i 6054 loss 1.1846245222091676\n",
      "epoch 135 i 7054 loss 1.1854269614219666\n",
      "epoch 135 i 8054 loss 1.1842195539474487\n",
      "epoch 135 i 9054 loss 1.183354977607727\n",
      "epoch 135 i 10054 loss 1.1823097945451737\n",
      "epoch 135 i 11054 loss 1.1843270800113679\n",
      "epoch 135 i 12054 loss 1.1838772035837173\n",
      "epoch 135 loss 1.239645004272461: ree of his circle for the addition cendomination starting for writing see are consists of mountain e\n",
      "epoch 136 i 847 loss 1.1822482089996338\n",
      "epoch 136 i 1847 loss 1.1830253318548203\n",
      "epoch 136 i 2847 loss 1.1832260140180588\n",
      "epoch 136 i 3847 loss 1.1831598353385926\n",
      "epoch 136 i 4847 loss 1.1822898333072662\n",
      "epoch 136 i 5847 loss 1.1834226655960083\n",
      "epoch 136 i 6847 loss 1.1839215382337571\n",
      "epoch 136 i 7847 loss 1.1823331277370452\n",
      "epoch 136 i 8847 loss 1.1835868808031083\n",
      "epoch 136 i 9847 loss 1.1820827285051345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, optim, loss_fn, encoded, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [77], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, loss_fn, data, epochs, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y_hat\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 27\u001b[0m lossi\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     28\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optim, loss_fn, encoded, epochs=1000, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act th\n",
      "output: tndm h st af ginalid tn t shrm tf t ost aovst oned tsein t anrly tark ng toass aetioat  an ludeng the cesurs  tf the cnglish aeselution and the stmd aoltt ld af the cieech aeselution aiice eahe chrm tn aeall one  tn t srrui  ive tis th tescribe and antithe\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "input = encoded[0:256]\n",
    "output = model(torch.tensor(input).to(\"cuda\").unsqueeze(0))\n",
    "token_output = torch.softmax(output, dim=-1).argmax(dim=-1)[0]\n",
    "print(f' input: {\"\".join([chars[i] for i in input])}')\n",
    "print(f'output: {\"\".join([chars[i] for i in token_output])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
